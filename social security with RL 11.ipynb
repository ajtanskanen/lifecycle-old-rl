{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforced learning, työllistyminen ja Suomen sosiaaliturva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tässä tehdään laskelmat artikkelia varten. Käytössä on gym-ympäristö _unemployment-v1_ , johon on toteutettu yksityiskohtaisempi työttömyysturvamalli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Colab, install fin_benefits and unemployment-gym from Github\n",
    "#!pip install -q git+https://github.com/ajtanskanen/benefits.git  \n",
    "#!pip install -q git+https://github.com/ajtanskanen/econogym.git\n",
    "#!pip install -q git+https://github.com/ajtanskanen/lifecycle-rl.git\n",
    "\n",
    "# and then restart kernel\n",
    "  \n",
    "  # For a specific version:\n",
    "#!pip install tensorflow==1.15\n",
    "#!pip install stable-baselines==2.8\n",
    "  \n",
    "# restart kernel after running pip's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then load all modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lifecycle_rl import Lifecycle\n",
    "\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "# varoitukset piiloon (Stable baseline ei ole vielä Tensorflow 2.0-yhteensopiva, ja Tensorflow 1.5 valittaa paljon)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pop_size=10_000\n",
    "size1=20_000_000\n",
    "size2=50_000_000\n",
    "batch1=1\n",
    "batch2=1000\n",
    "deterministic=False # use deterministic prediction (True) or probabilitic prediction (False)\n",
    "mortality=False # include mortality in computations\n",
    "randomness=True # include randomness in the state transitions (e.g., maternity leave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Nykymalli \n",
    "\n",
    "Lasketaan työllisyysasteet nykymallissa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No mortality included\n",
      "Parameters of lifecycle:\n",
      "timestep 0.25\n",
      "gamma 0.9793703613355593 (0.9200000000000003 per anno)\n",
      "min_age 20\n",
      "max_age 70\n",
      "min_retirementage 65\n",
      "max_retirementage 70\n",
      "ansiopvraha_kesto300 None\n",
      "ansiopvraha_kesto400 None\n",
      "ansiopvraha_toe None\n",
      "perustulo False\n",
      "karenssi_kesto 0.25\n",
      "mortality False\n",
      "randomness True\n",
      "deterministic False\n",
      "\n",
      "include_putki None\n",
      "step 0.25\n",
      "\n",
      "train...\n",
      "phase 1\n",
      "training...\n",
      "---------------------------------\n",
      "| explained_variance | 0.00272  |\n",
      "| fps                | 1774     |\n",
      "| nupdates           | 1        |\n",
      "| policy_entropy     | 1.39     |\n",
      "| policy_loss        | 52.4     |\n",
      "| total_timesteps    | 0        |\n",
      "| value_loss         | 1.53e+03 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.624    |\n",
      "| fps                | 2305     |\n",
      "| nupdates           | 1000     |\n",
      "| policy_entropy     | 0.969    |\n",
      "| policy_loss        | -0.105   |\n",
      "| total_timesteps    | 2458539  |\n",
      "| value_loss         | 24.6     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.944    |\n",
      "| fps                | 2320     |\n",
      "| nupdates           | 2000     |\n",
      "| policy_entropy     | 0.717    |\n",
      "| policy_loss        | 0.169    |\n",
      "| total_timesteps    | 4919539  |\n",
      "| value_loss         | 5.46     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.714    |\n",
      "| fps                | 2326     |\n",
      "| nupdates           | 3000     |\n",
      "| policy_entropy     | 0.597    |\n",
      "| policy_loss        | -0.123   |\n",
      "| total_timesteps    | 7380539  |\n",
      "| value_loss         | 14.3     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.776    |\n",
      "| fps                | 2315     |\n",
      "| nupdates           | 4000     |\n",
      "| policy_entropy     | 0.341    |\n",
      "| policy_loss        | -0.0337  |\n",
      "| total_timesteps    | 9841539  |\n",
      "| value_loss         | 11.9     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.867    |\n",
      "| fps                | 2260     |\n",
      "| nupdates           | 5000     |\n",
      "| policy_entropy     | 0.449    |\n",
      "| policy_loss        | 0.0964   |\n",
      "| total_timesteps    | 12302539 |\n",
      "| value_loss         | 10.8     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.838    |\n",
      "| fps                | 2224     |\n",
      "| nupdates           | 6000     |\n",
      "| policy_entropy     | 0.329    |\n",
      "| policy_loss        | -0.26    |\n",
      "| total_timesteps    | 14763539 |\n",
      "| value_loss         | 12.5     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.968    |\n",
      "| fps                | 2213     |\n",
      "| nupdates           | 7000     |\n",
      "| policy_entropy     | 0.397    |\n",
      "| policy_loss        | 0.266    |\n",
      "| total_timesteps    | 17224539 |\n",
      "| value_loss         | 2.1      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.836    |\n",
      "| fps                | 2170     |\n",
      "| nupdates           | 8000     |\n",
      "| policy_entropy     | 0.52     |\n",
      "| policy_loss        | -0.2     |\n",
      "| total_timesteps    | 19685539 |\n",
      "| value_loss         | 19.6     |\n",
      "---------------------------------\n",
      "done\n",
      "phase 2\n",
      "init vecmonitor:  tmp/monitor.csv\n",
      "training...\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 222.65306 |\n",
      "| explained_variance | 0.857     |\n",
      "| fps                | 3138      |\n",
      "| nupdates           | 1         |\n",
      "| policy_entropy     | 0.452     |\n",
      "| policy_loss        | -0.0334   |\n",
      "| total_timesteps    | 0         |\n",
      "| value_loss         | 11.9      |\n",
      "----------------------------------\n",
      "412452 timesteps\n",
      "New best mean reward: 220.23 - Last best reward per episode: -inf\n",
      "saved as  saved/best_perus\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 219.2051 |\n",
      "| explained_variance | 0.826    |\n",
      "| fps                | 3159     |\n",
      "| nupdates           | 2        |\n",
      "| policy_entropy     | 0.469    |\n",
      "| policy_loss        | 3.63     |\n",
      "| total_timesteps    | 206641   |\n",
      "| value_loss         | 79.9     |\n",
      "---------------------------------\n",
      "619884 timesteps\n",
      "New best mean reward: 221.93 - Last best reward per episode: 220.23\n",
      "saved as  saved/best_perus\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.61292 |\n",
      "| explained_variance | 0.797     |\n",
      "| fps                | 3179      |\n",
      "| nupdates           | 3         |\n",
      "| policy_entropy     | 0.481     |\n",
      "| policy_loss        | 3.02      |\n",
      "| total_timesteps    | 413282    |\n",
      "| value_loss         | 56.8      |\n",
      "----------------------------------\n",
      "824904 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 221.11\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 222.91617 |\n",
      "| explained_variance | 0.787     |\n",
      "| fps                | 3190      |\n",
      "| nupdates           | 4         |\n",
      "| policy_entropy     | 0.51      |\n",
      "| policy_loss        | -2.07     |\n",
      "| total_timesteps    | 619923    |\n",
      "| value_loss         | 34.4      |\n",
      "----------------------------------\n",
      "1032336 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.70\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.16467 |\n",
      "| explained_variance | 0.811     |\n",
      "| fps                | 3194      |\n",
      "| nupdates           | 5         |\n",
      "| policy_entropy     | 0.518     |\n",
      "| policy_loss        | -2.49     |\n",
      "| total_timesteps    | 826564    |\n",
      "| value_loss         | 39.3      |\n",
      "----------------------------------\n",
      "1239768 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.14\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 217.07945 |\n",
      "| explained_variance | 0.817     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 6         |\n",
      "| policy_entropy     | 0.526     |\n",
      "| policy_loss        | 1.55      |\n",
      "| total_timesteps    | 1033205   |\n",
      "| value_loss         | 24.1      |\n",
      "----------------------------------\n",
      "1444788 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.01\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.22588 |\n",
      "| explained_variance | 0.814     |\n",
      "| fps                | 3200      |\n",
      "| nupdates           | 7         |\n",
      "| policy_entropy     | 0.531     |\n",
      "| policy_loss        | 0.814     |\n",
      "| total_timesteps    | 1239846   |\n",
      "| value_loss         | 16.6      |\n",
      "----------------------------------\n",
      "1652220 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.73\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.30557 |\n",
      "| explained_variance | 0.799     |\n",
      "| fps                | 3202      |\n",
      "| nupdates           | 8         |\n",
      "| policy_entropy     | 0.546     |\n",
      "| policy_loss        | -4.43     |\n",
      "| total_timesteps    | 1446487   |\n",
      "| value_loss         | 86.5      |\n",
      "----------------------------------\n",
      "1859652 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 217.91\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 220.0851 |\n",
      "| explained_variance | 0.734    |\n",
      "| fps                | 3203     |\n",
      "| nupdates           | 9        |\n",
      "| policy_entropy     | 0.546    |\n",
      "| policy_loss        | -4.43    |\n",
      "| total_timesteps    | 1653128  |\n",
      "| value_loss         | 93.9     |\n",
      "---------------------------------\n",
      "2064672 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.95\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 215.45615 |\n",
      "| explained_variance | 0.784     |\n",
      "| fps                | 3201      |\n",
      "| nupdates           | 10        |\n",
      "| policy_entropy     | 0.545     |\n",
      "| policy_loss        | 0.263     |\n",
      "| total_timesteps    | 1859769   |\n",
      "| value_loss         | 17.2      |\n",
      "----------------------------------\n",
      "2272104 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 217.37\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 214.8826 |\n",
      "| explained_variance | 0.714    |\n",
      "| fps                | 3201     |\n",
      "| nupdates           | 11       |\n",
      "| policy_entropy     | 0.557    |\n",
      "| policy_loss        | 1.06     |\n",
      "| total_timesteps    | 2066410  |\n",
      "| value_loss         | 31.1     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2479536 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.11\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.43027 |\n",
      "| explained_variance | 0.674     |\n",
      "| fps                | 3199      |\n",
      "| nupdates           | 12        |\n",
      "| policy_entropy     | 0.564     |\n",
      "| policy_loss        | -2.16     |\n",
      "| total_timesteps    | 2273051   |\n",
      "| value_loss         | 35.8      |\n",
      "----------------------------------\n",
      "2684556 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 217.40\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 216.02798 |\n",
      "| explained_variance | 0.676     |\n",
      "| fps                | 3199      |\n",
      "| nupdates           | 13        |\n",
      "| policy_entropy     | 0.568     |\n",
      "| policy_loss        | -2.24     |\n",
      "| total_timesteps    | 2479692   |\n",
      "| value_loss         | 37.6      |\n",
      "----------------------------------\n",
      "2891988 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.95\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.63965 |\n",
      "| explained_variance | 0.67      |\n",
      "| fps                | 3198      |\n",
      "| nupdates           | 14        |\n",
      "| policy_entropy     | 0.564     |\n",
      "| policy_loss        | -2.02     |\n",
      "| total_timesteps    | 2686333   |\n",
      "| value_loss         | 32.9      |\n",
      "----------------------------------\n",
      "3099420 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 217.51\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.02676 |\n",
      "| explained_variance | 0.7       |\n",
      "| fps                | 3198      |\n",
      "| nupdates           | 15        |\n",
      "| policy_entropy     | 0.575     |\n",
      "| policy_loss        | -2.19     |\n",
      "| total_timesteps    | 2892974   |\n",
      "| value_loss         | 33.1      |\n",
      "----------------------------------\n",
      "3304440 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.93\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.36287 |\n",
      "| explained_variance | 0.692     |\n",
      "| fps                | 3198      |\n",
      "| nupdates           | 16        |\n",
      "| policy_entropy     | 0.562     |\n",
      "| policy_loss        | -1.96     |\n",
      "| total_timesteps    | 3099615   |\n",
      "| value_loss         | 30.9      |\n",
      "----------------------------------\n",
      "3511872 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.54\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 215.26233 |\n",
      "| explained_variance | 0.716     |\n",
      "| fps                | 3192      |\n",
      "| nupdates           | 17        |\n",
      "| policy_entropy     | 0.571     |\n",
      "| policy_loss        | -2.03     |\n",
      "| total_timesteps    | 3306256   |\n",
      "| value_loss         | 30.2      |\n",
      "----------------------------------\n",
      "3719304 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.81\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 219.6781 |\n",
      "| explained_variance | 0.707    |\n",
      "| fps                | 3183     |\n",
      "| nupdates           | 18       |\n",
      "| policy_entropy     | 0.562    |\n",
      "| policy_loss        | -1.87    |\n",
      "| total_timesteps    | 3512897  |\n",
      "| value_loss         | 29       |\n",
      "---------------------------------\n",
      "3924324 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.66\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 216.151  |\n",
      "| explained_variance | 0.723    |\n",
      "| fps                | 3179     |\n",
      "| nupdates           | 19       |\n",
      "| policy_entropy     | 0.572    |\n",
      "| policy_loss        | -1.91    |\n",
      "| total_timesteps    | 3719538  |\n",
      "| value_loss         | 28.2     |\n",
      "---------------------------------\n",
      "4131756 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.24\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.51347 |\n",
      "| explained_variance | 0.74      |\n",
      "| fps                | 3179      |\n",
      "| nupdates           | 20        |\n",
      "| policy_entropy     | 0.572     |\n",
      "| policy_loss        | -1.88     |\n",
      "| total_timesteps    | 3926179   |\n",
      "| value_loss         | 27.2      |\n",
      "----------------------------------\n",
      "4339188 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.37\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.43541 |\n",
      "| explained_variance | 0.748     |\n",
      "| fps                | 3179      |\n",
      "| nupdates           | 21        |\n",
      "| policy_entropy     | 0.577     |\n",
      "| policy_loss        | -1.75     |\n",
      "| total_timesteps    | 4132820   |\n",
      "| value_loss         | 25.1      |\n",
      "----------------------------------\n",
      "4544208 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.62\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.48439 |\n",
      "| explained_variance | 0.751     |\n",
      "| fps                | 3180      |\n",
      "| nupdates           | 22        |\n",
      "| policy_entropy     | 0.572     |\n",
      "| policy_loss        | -1.54     |\n",
      "| total_timesteps    | 4339461   |\n",
      "| value_loss         | 22.3      |\n",
      "----------------------------------\n",
      "4751640 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.94\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.71436 |\n",
      "| explained_variance | 0.764     |\n",
      "| fps                | 3181      |\n",
      "| nupdates           | 23        |\n",
      "| policy_entropy     | 0.568     |\n",
      "| policy_loss        | -1.43     |\n",
      "| total_timesteps    | 4546102   |\n",
      "| value_loss         | 21.9      |\n",
      "----------------------------------\n",
      "4959072 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.83\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.84178 |\n",
      "| explained_variance | 0.783     |\n",
      "| fps                | 3181      |\n",
      "| nupdates           | 24        |\n",
      "| policy_entropy     | 0.572     |\n",
      "| policy_loss        | -1.23     |\n",
      "| total_timesteps    | 4752743   |\n",
      "| value_loss         | 18.4      |\n",
      "----------------------------------\n",
      "5164092 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.54\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 216.32076 |\n",
      "| explained_variance | 0.802     |\n",
      "| fps                | 3181      |\n",
      "| nupdates           | 25        |\n",
      "| policy_entropy     | 0.582     |\n",
      "| policy_loss        | -1.19     |\n",
      "| total_timesteps    | 4959384   |\n",
      "| value_loss         | 17.5      |\n",
      "----------------------------------\n",
      "5371524 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.32\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.87968 |\n",
      "| explained_variance | 0.805     |\n",
      "| fps                | 3182      |\n",
      "| nupdates           | 26        |\n",
      "| policy_entropy     | 0.581     |\n",
      "| policy_loss        | -1.06     |\n",
      "| total_timesteps    | 5166025   |\n",
      "| value_loss         | 17.9      |\n",
      "----------------------------------\n",
      "5578956 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.05\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 217.008  |\n",
      "| explained_variance | 0.813    |\n",
      "| fps                | 3182     |\n",
      "| nupdates           | 27       |\n",
      "| policy_entropy     | 0.579    |\n",
      "| policy_loss        | -0.799   |\n",
      "| total_timesteps    | 5372666  |\n",
      "| value_loss         | 15.8     |\n",
      "---------------------------------\n",
      "5783976 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 217.51\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 222.00336 |\n",
      "| explained_variance | 0.811     |\n",
      "| fps                | 3182      |\n",
      "| nupdates           | 28        |\n",
      "| policy_entropy     | 0.58      |\n",
      "| policy_loss        | -0.659    |\n",
      "| total_timesteps    | 5579307   |\n",
      "| value_loss         | 15.9      |\n",
      "----------------------------------\n",
      "5991408 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.45\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.32639 |\n",
      "| explained_variance | 0.82      |\n",
      "| fps                | 3183      |\n",
      "| nupdates           | 29        |\n",
      "| policy_entropy     | 0.571     |\n",
      "| policy_loss        | -0.352    |\n",
      "| total_timesteps    | 5785948   |\n",
      "| value_loss         | 14        |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6198840 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.54\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.35402 |\n",
      "| explained_variance | 0.82      |\n",
      "| fps                | 3183      |\n",
      "| nupdates           | 30        |\n",
      "| policy_entropy     | 0.58      |\n",
      "| policy_loss        | -0.191    |\n",
      "| total_timesteps    | 5992589   |\n",
      "| value_loss         | 14        |\n",
      "----------------------------------\n",
      "6403860 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.81\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 216.49677 |\n",
      "| explained_variance | 0.844     |\n",
      "| fps                | 3183      |\n",
      "| nupdates           | 31        |\n",
      "| policy_entropy     | 0.582     |\n",
      "| policy_loss        | 0.0206    |\n",
      "| total_timesteps    | 6199230   |\n",
      "| value_loss         | 12.6      |\n",
      "----------------------------------\n",
      "6611292 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.19\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.74792 |\n",
      "| explained_variance | 0.838     |\n",
      "| fps                | 3184      |\n",
      "| nupdates           | 32        |\n",
      "| policy_entropy     | 0.574     |\n",
      "| policy_loss        | 0.264     |\n",
      "| total_timesteps    | 6405871   |\n",
      "| value_loss         | 12.4      |\n",
      "----------------------------------\n",
      "6818724 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.74\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.86523 |\n",
      "| explained_variance | 0.833     |\n",
      "| fps                | 3184      |\n",
      "| nupdates           | 33        |\n",
      "| policy_entropy     | 0.574     |\n",
      "| policy_loss        | 0.396     |\n",
      "| total_timesteps    | 6612512   |\n",
      "| value_loss         | 13.4      |\n",
      "----------------------------------\n",
      "7023744 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.00\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 216.23564 |\n",
      "| explained_variance | 0.828     |\n",
      "| fps                | 3184      |\n",
      "| nupdates           | 34        |\n",
      "| policy_entropy     | 0.577     |\n",
      "| policy_loss        | 0.533     |\n",
      "| total_timesteps    | 6819153   |\n",
      "| value_loss         | 14.2      |\n",
      "----------------------------------\n",
      "7231176 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.17\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 218.9671 |\n",
      "| explained_variance | 0.834    |\n",
      "| fps                | 3185     |\n",
      "| nupdates           | 35       |\n",
      "| policy_entropy     | 0.575    |\n",
      "| policy_loss        | 0.636    |\n",
      "| total_timesteps    | 7025794  |\n",
      "| value_loss         | 13.8     |\n",
      "---------------------------------\n",
      "7438608 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 217.67\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.64293 |\n",
      "| explained_variance | 0.83      |\n",
      "| fps                | 3185      |\n",
      "| nupdates           | 36        |\n",
      "| policy_entropy     | 0.582     |\n",
      "| policy_loss        | 0.702     |\n",
      "| total_timesteps    | 7232435   |\n",
      "| value_loss         | 14.9      |\n",
      "----------------------------------\n",
      "7643628 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 217.56\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.25043 |\n",
      "| explained_variance | 0.829     |\n",
      "| fps                | 3186      |\n",
      "| nupdates           | 37        |\n",
      "| policy_entropy     | 0.587     |\n",
      "| policy_loss        | 0.798     |\n",
      "| total_timesteps    | 7439076   |\n",
      "| value_loss         | 15.1      |\n",
      "----------------------------------\n",
      "7851060 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.97\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.22638 |\n",
      "| explained_variance | 0.845     |\n",
      "| fps                | 3186      |\n",
      "| nupdates           | 38        |\n",
      "| policy_entropy     | 0.584     |\n",
      "| policy_loss        | 0.858     |\n",
      "| total_timesteps    | 7645717   |\n",
      "| value_loss         | 13.7      |\n",
      "----------------------------------\n",
      "8058492 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.10\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 222.31465 |\n",
      "| explained_variance | 0.821     |\n",
      "| fps                | 3186      |\n",
      "| nupdates           | 39        |\n",
      "| policy_entropy     | 0.581     |\n",
      "| policy_loss        | 0.839     |\n",
      "| total_timesteps    | 7852358   |\n",
      "| value_loss         | 15.4      |\n",
      "----------------------------------\n",
      "8263512 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.83\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 218.6774 |\n",
      "| explained_variance | 0.835    |\n",
      "| fps                | 3187     |\n",
      "| nupdates           | 40       |\n",
      "| policy_entropy     | 0.576    |\n",
      "| policy_loss        | 0.821    |\n",
      "| total_timesteps    | 8058999  |\n",
      "| value_loss         | 13.9     |\n",
      "---------------------------------\n",
      "8470944 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 217.94\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.79822 |\n",
      "| explained_variance | 0.83      |\n",
      "| fps                | 3187      |\n",
      "| nupdates           | 41        |\n",
      "| policy_entropy     | 0.582     |\n",
      "| policy_loss        | 0.769     |\n",
      "| total_timesteps    | 8265640   |\n",
      "| value_loss         | 15.1      |\n",
      "----------------------------------\n",
      "8678376 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.07\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.86012 |\n",
      "| explained_variance | 0.837     |\n",
      "| fps                | 3187      |\n",
      "| nupdates           | 42        |\n",
      "| policy_entropy     | 0.578     |\n",
      "| policy_loss        | 0.667     |\n",
      "| total_timesteps    | 8472281   |\n",
      "| value_loss         | 14.2      |\n",
      "----------------------------------\n",
      "8883396 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.82\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 225.62094 |\n",
      "| explained_variance | 0.831     |\n",
      "| fps                | 3187      |\n",
      "| nupdates           | 43        |\n",
      "| policy_entropy     | 0.57      |\n",
      "| policy_loss        | 0.631     |\n",
      "| total_timesteps    | 8678922   |\n",
      "| value_loss         | 13.2      |\n",
      "----------------------------------\n",
      "9090828 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.22\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.04274 |\n",
      "| explained_variance | 0.83      |\n",
      "| fps                | 3188      |\n",
      "| nupdates           | 44        |\n",
      "| policy_entropy     | 0.581     |\n",
      "| policy_loss        | 0.486     |\n",
      "| total_timesteps    | 8885563   |\n",
      "| value_loss         | 14.1      |\n",
      "----------------------------------\n",
      "9298260 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.00\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 213.42924 |\n",
      "| explained_variance | 0.85      |\n",
      "| fps                | 3188      |\n",
      "| nupdates           | 45        |\n",
      "| policy_entropy     | 0.579     |\n",
      "| policy_loss        | 0.437     |\n",
      "| total_timesteps    | 9092204   |\n",
      "| value_loss         | 12.2      |\n",
      "----------------------------------\n",
      "9503280 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.01\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 216.30695 |\n",
      "| explained_variance | 0.852     |\n",
      "| fps                | 3188      |\n",
      "| nupdates           | 46        |\n",
      "| policy_entropy     | 0.582     |\n",
      "| policy_loss        | 0.26      |\n",
      "| total_timesteps    | 9298845   |\n",
      "| value_loss         | 12.3      |\n",
      "----------------------------------\n",
      "9710712 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.18\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.00725 |\n",
      "| explained_variance | 0.852     |\n",
      "| fps                | 3188      |\n",
      "| nupdates           | 47        |\n",
      "| policy_entropy     | 0.577     |\n",
      "| policy_loss        | 0.167     |\n",
      "| total_timesteps    | 9505486   |\n",
      "| value_loss         | 11.6      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9918144 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.25\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 215.89636 |\n",
      "| explained_variance | 0.839     |\n",
      "| fps                | 3189      |\n",
      "| nupdates           | 48        |\n",
      "| policy_entropy     | 0.575     |\n",
      "| policy_loss        | 0.00503   |\n",
      "| total_timesteps    | 9712127   |\n",
      "| value_loss         | 12.3      |\n",
      "----------------------------------\n",
      "10123164 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.01\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.70355 |\n",
      "| explained_variance | 0.825     |\n",
      "| fps                | 3189      |\n",
      "| nupdates           | 49        |\n",
      "| policy_entropy     | 0.57      |\n",
      "| policy_loss        | -0.147    |\n",
      "| total_timesteps    | 9918768   |\n",
      "| value_loss         | 13.5      |\n",
      "----------------------------------\n",
      "10330596 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.43\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.40024 |\n",
      "| explained_variance | 0.849     |\n",
      "| fps                | 3189      |\n",
      "| nupdates           | 50        |\n",
      "| policy_entropy     | 0.579     |\n",
      "| policy_loss        | -0.233    |\n",
      "| total_timesteps    | 10125409  |\n",
      "| value_loss         | 11.9      |\n",
      "----------------------------------\n",
      "10538028 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 217.65\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 222.66684 |\n",
      "| explained_variance | 0.844     |\n",
      "| fps                | 3189      |\n",
      "| nupdates           | 51        |\n",
      "| policy_entropy     | 0.58      |\n",
      "| policy_loss        | -0.392    |\n",
      "| total_timesteps    | 10332050  |\n",
      "| value_loss         | 13.2      |\n",
      "----------------------------------\n",
      "10743048 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.31\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 215.9317 |\n",
      "| explained_variance | 0.846    |\n",
      "| fps                | 3190     |\n",
      "| nupdates           | 52       |\n",
      "| policy_entropy     | 0.572    |\n",
      "| policy_loss        | -0.446   |\n",
      "| total_timesteps    | 10538691 |\n",
      "| value_loss         | 12.6     |\n",
      "---------------------------------\n",
      "10950480 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.82\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.80748 |\n",
      "| explained_variance | 0.843     |\n",
      "| fps                | 3190      |\n",
      "| nupdates           | 53        |\n",
      "| policy_entropy     | 0.574     |\n",
      "| policy_loss        | -0.464    |\n",
      "| total_timesteps    | 10745332  |\n",
      "| value_loss         | 12.5      |\n",
      "----------------------------------\n",
      "11157912 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.06\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 215.14458 |\n",
      "| explained_variance | 0.84      |\n",
      "| fps                | 3190      |\n",
      "| nupdates           | 54        |\n",
      "| policy_entropy     | 0.571     |\n",
      "| policy_loss        | -0.547    |\n",
      "| total_timesteps    | 10951973  |\n",
      "| value_loss         | 13.5      |\n",
      "----------------------------------\n",
      "11362932 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.28\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.22397 |\n",
      "| explained_variance | 0.856     |\n",
      "| fps                | 3190      |\n",
      "| nupdates           | 55        |\n",
      "| policy_entropy     | 0.579     |\n",
      "| policy_loss        | -0.515    |\n",
      "| total_timesteps    | 11158614  |\n",
      "| value_loss         | 11.7      |\n",
      "----------------------------------\n",
      "11570364 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.96\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 222.40375 |\n",
      "| explained_variance | 0.85      |\n",
      "| fps                | 3190      |\n",
      "| nupdates           | 56        |\n",
      "| policy_entropy     | 0.571     |\n",
      "| policy_loss        | -0.53     |\n",
      "| total_timesteps    | 11365255  |\n",
      "| value_loss         | 12.4      |\n",
      "----------------------------------\n",
      "11777796 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.81\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 218.221  |\n",
      "| explained_variance | 0.848    |\n",
      "| fps                | 3190     |\n",
      "| nupdates           | 57       |\n",
      "| policy_entropy     | 0.572    |\n",
      "| policy_loss        | -0.539   |\n",
      "| total_timesteps    | 11571896 |\n",
      "| value_loss         | 12.7     |\n",
      "---------------------------------\n",
      "11982816 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 217.71\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 219.6303 |\n",
      "| explained_variance | 0.841    |\n",
      "| fps                | 3191     |\n",
      "| nupdates           | 58       |\n",
      "| policy_entropy     | 0.58     |\n",
      "| policy_loss        | -0.547   |\n",
      "| total_timesteps    | 11778537 |\n",
      "| value_loss         | 13.8     |\n",
      "---------------------------------\n",
      "12190248 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.83\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.31357 |\n",
      "| explained_variance | 0.856     |\n",
      "| fps                | 3191      |\n",
      "| nupdates           | 59        |\n",
      "| policy_entropy     | 0.573     |\n",
      "| policy_loss        | -0.379    |\n",
      "| total_timesteps    | 11985178  |\n",
      "| value_loss         | 11        |\n",
      "----------------------------------\n",
      "12397680 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.63\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.62843 |\n",
      "| explained_variance | 0.849     |\n",
      "| fps                | 3191      |\n",
      "| nupdates           | 60        |\n",
      "| policy_entropy     | 0.575     |\n",
      "| policy_loss        | -0.389    |\n",
      "| total_timesteps    | 12191819  |\n",
      "| value_loss         | 12.4      |\n",
      "----------------------------------\n",
      "12602700 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.51\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.29315 |\n",
      "| explained_variance | 0.849     |\n",
      "| fps                | 3191      |\n",
      "| nupdates           | 61        |\n",
      "| policy_entropy     | 0.574     |\n",
      "| policy_loss        | -0.287    |\n",
      "| total_timesteps    | 12398460  |\n",
      "| value_loss         | 11.7      |\n",
      "----------------------------------\n",
      "12810132 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.29\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.42235 |\n",
      "| explained_variance | 0.852     |\n",
      "| fps                | 3191      |\n",
      "| nupdates           | 62        |\n",
      "| policy_entropy     | 0.571     |\n",
      "| policy_loss        | -0.153    |\n",
      "| total_timesteps    | 12605101  |\n",
      "| value_loss         | 11.1      |\n",
      "----------------------------------\n",
      "13017564 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.85\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.27274 |\n",
      "| explained_variance | 0.856     |\n",
      "| fps                | 3191      |\n",
      "| nupdates           | 63        |\n",
      "| policy_entropy     | 0.58      |\n",
      "| policy_loss        | -0.0819   |\n",
      "| total_timesteps    | 12811742  |\n",
      "| value_loss         | 11.4      |\n",
      "----------------------------------\n",
      "13222584 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.80\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.17552 |\n",
      "| explained_variance | 0.86      |\n",
      "| fps                | 3192      |\n",
      "| nupdates           | 64        |\n",
      "| policy_entropy     | 0.582     |\n",
      "| policy_loss        | 0.0441    |\n",
      "| total_timesteps    | 13018383  |\n",
      "| value_loss         | 10.7      |\n",
      "----------------------------------\n",
      "13430016 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.87\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.31415 |\n",
      "| explained_variance | 0.854     |\n",
      "| fps                | 3192      |\n",
      "| nupdates           | 65        |\n",
      "| policy_entropy     | 0.575     |\n",
      "| policy_loss        | 0.144     |\n",
      "| total_timesteps    | 13225024  |\n",
      "| value_loss         | 10.8      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13637448 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.21\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 223.66962 |\n",
      "| explained_variance | 0.838     |\n",
      "| fps                | 3192      |\n",
      "| nupdates           | 66        |\n",
      "| policy_entropy     | 0.568     |\n",
      "| policy_loss        | 0.2       |\n",
      "| total_timesteps    | 13431665  |\n",
      "| value_loss         | 12.3      |\n",
      "----------------------------------\n",
      "13844880 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.33\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.83296 |\n",
      "| explained_variance | 0.856     |\n",
      "| fps                | 3192      |\n",
      "| nupdates           | 67        |\n",
      "| policy_entropy     | 0.574     |\n",
      "| policy_loss        | 0.303     |\n",
      "| total_timesteps    | 13638306  |\n",
      "| value_loss         | 11.2      |\n",
      "----------------------------------\n",
      "14049900 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 217.62\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.28015 |\n",
      "| explained_variance | 0.845     |\n",
      "| fps                | 3192      |\n",
      "| nupdates           | 68        |\n",
      "| policy_entropy     | 0.588     |\n",
      "| policy_loss        | 0.271     |\n",
      "| total_timesteps    | 13844947  |\n",
      "| value_loss         | 12.7      |\n",
      "----------------------------------\n",
      "14257332 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.32\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 216.97198 |\n",
      "| explained_variance | 0.848     |\n",
      "| fps                | 3192      |\n",
      "| nupdates           | 69        |\n",
      "| policy_entropy     | 0.582     |\n",
      "| policy_loss        | 0.338     |\n",
      "| total_timesteps    | 14051588  |\n",
      "| value_loss         | 12.4      |\n",
      "----------------------------------\n",
      "14464764 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.10\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.14365 |\n",
      "| explained_variance | 0.861     |\n",
      "| fps                | 3192      |\n",
      "| nupdates           | 70        |\n",
      "| policy_entropy     | 0.588     |\n",
      "| policy_loss        | 0.407     |\n",
      "| total_timesteps    | 14258229  |\n",
      "| value_loss         | 11.3      |\n",
      "----------------------------------\n",
      "14669784 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.76\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.23656 |\n",
      "| explained_variance | 0.863     |\n",
      "| fps                | 3192      |\n",
      "| nupdates           | 71        |\n",
      "| policy_entropy     | 0.59      |\n",
      "| policy_loss        | 0.364     |\n",
      "| total_timesteps    | 14464870  |\n",
      "| value_loss         | 11.2      |\n",
      "----------------------------------\n",
      "14877216 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.68\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 217.0775 |\n",
      "| explained_variance | 0.857    |\n",
      "| fps                | 3192     |\n",
      "| nupdates           | 72       |\n",
      "| policy_entropy     | 0.581    |\n",
      "| policy_loss        | 0.353    |\n",
      "| total_timesteps    | 14671511 |\n",
      "| value_loss         | 11.5     |\n",
      "---------------------------------\n",
      "15084648 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.68\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 222.87799 |\n",
      "| explained_variance | 0.859     |\n",
      "| fps                | 3192      |\n",
      "| nupdates           | 73        |\n",
      "| policy_entropy     | 0.582     |\n",
      "| policy_loss        | 0.309     |\n",
      "| total_timesteps    | 14878152  |\n",
      "| value_loss         | 11.3      |\n",
      "----------------------------------\n",
      "15289668 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.02\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 220.5795 |\n",
      "| explained_variance | 0.836    |\n",
      "| fps                | 3192     |\n",
      "| nupdates           | 74       |\n",
      "| policy_entropy     | 0.579    |\n",
      "| policy_loss        | 0.231    |\n",
      "| total_timesteps    | 15084793 |\n",
      "| value_loss         | 12.4     |\n",
      "---------------------------------\n",
      "15497100 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.19\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 217.73068 |\n",
      "| explained_variance | 0.853     |\n",
      "| fps                | 3192      |\n",
      "| nupdates           | 75        |\n",
      "| policy_entropy     | 0.582     |\n",
      "| policy_loss        | 0.172     |\n",
      "| total_timesteps    | 15291434  |\n",
      "| value_loss         | 11.7      |\n",
      "----------------------------------\n",
      "15704532 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.82\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 217.11473 |\n",
      "| explained_variance | 0.852     |\n",
      "| fps                | 3192      |\n",
      "| nupdates           | 76        |\n",
      "| policy_entropy     | 0.583     |\n",
      "| policy_loss        | 0.0851    |\n",
      "| total_timesteps    | 15498075  |\n",
      "| value_loss         | 11.6      |\n",
      "----------------------------------\n",
      "15909552 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.41\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.63979 |\n",
      "| explained_variance | 0.861     |\n",
      "| fps                | 3192      |\n",
      "| nupdates           | 77        |\n",
      "| policy_entropy     | 0.581     |\n",
      "| policy_loss        | 0.00962   |\n",
      "| total_timesteps    | 15704716  |\n",
      "| value_loss         | 10.7      |\n",
      "----------------------------------\n",
      "16116984 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.40\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.48181 |\n",
      "| explained_variance | 0.852     |\n",
      "| fps                | 3193      |\n",
      "| nupdates           | 78        |\n",
      "| policy_entropy     | 0.584     |\n",
      "| policy_loss        | -0.146    |\n",
      "| total_timesteps    | 15911357  |\n",
      "| value_loss         | 12.2      |\n",
      "----------------------------------\n",
      "16324416 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.78\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.42671 |\n",
      "| explained_variance | 0.841     |\n",
      "| fps                | 3193      |\n",
      "| nupdates           | 79        |\n",
      "| policy_entropy     | 0.573     |\n",
      "| policy_loss        | -0.167    |\n",
      "| total_timesteps    | 16117998  |\n",
      "| value_loss         | 12.2      |\n",
      "----------------------------------\n",
      "16529436 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.58\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.51152 |\n",
      "| explained_variance | 0.872     |\n",
      "| fps                | 3193      |\n",
      "| nupdates           | 80        |\n",
      "| policy_entropy     | 0.587     |\n",
      "| policy_loss        | -0.178    |\n",
      "| total_timesteps    | 16324639  |\n",
      "| value_loss         | 9.81      |\n",
      "----------------------------------\n",
      "16736868 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.23\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.69847 |\n",
      "| explained_variance | 0.856     |\n",
      "| fps                | 3193      |\n",
      "| nupdates           | 81        |\n",
      "| policy_entropy     | 0.574     |\n",
      "| policy_loss        | -0.235    |\n",
      "| total_timesteps    | 16531280  |\n",
      "| value_loss         | 10.9      |\n",
      "----------------------------------\n",
      "16944300 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.59\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.66003 |\n",
      "| explained_variance | 0.841     |\n",
      "| fps                | 3193      |\n",
      "| nupdates           | 82        |\n",
      "| policy_entropy     | 0.577     |\n",
      "| policy_loss        | -0.336    |\n",
      "| total_timesteps    | 16737921  |\n",
      "| value_loss         | 13        |\n",
      "----------------------------------\n",
      "17149320 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.21\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.90863 |\n",
      "| explained_variance | 0.847     |\n",
      "| fps                | 3193      |\n",
      "| nupdates           | 83        |\n",
      "| policy_entropy     | 0.579     |\n",
      "| policy_loss        | -0.313    |\n",
      "| total_timesteps    | 16944562  |\n",
      "| value_loss         | 12.2      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17356752 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.87\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.33588 |\n",
      "| explained_variance | 0.85      |\n",
      "| fps                | 3193      |\n",
      "| nupdates           | 84        |\n",
      "| policy_entropy     | 0.582     |\n",
      "| policy_loss        | -0.326    |\n",
      "| total_timesteps    | 17151203  |\n",
      "| value_loss         | 12.3      |\n",
      "----------------------------------\n",
      "17564184 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.95\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.86769 |\n",
      "| explained_variance | 0.85      |\n",
      "| fps                | 3193      |\n",
      "| nupdates           | 85        |\n",
      "| policy_entropy     | 0.58      |\n",
      "| policy_loss        | -0.289    |\n",
      "| total_timesteps    | 17357844  |\n",
      "| value_loss         | 11.9      |\n",
      "----------------------------------\n",
      "17769204 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 217.83\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.38644 |\n",
      "| explained_variance | 0.853     |\n",
      "| fps                | 3193      |\n",
      "| nupdates           | 86        |\n",
      "| policy_entropy     | 0.586     |\n",
      "| policy_loss        | -0.29     |\n",
      "| total_timesteps    | 17564485  |\n",
      "| value_loss         | 12.3      |\n",
      "----------------------------------\n",
      "17976636 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.73\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.33156 |\n",
      "| explained_variance | 0.85      |\n",
      "| fps                | 3193      |\n",
      "| nupdates           | 87        |\n",
      "| policy_entropy     | 0.581     |\n",
      "| policy_loss        | -0.23     |\n",
      "| total_timesteps    | 17771126  |\n",
      "| value_loss         | 12.3      |\n",
      "----------------------------------\n",
      "18184068 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.29\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 217.25648 |\n",
      "| explained_variance | 0.853     |\n",
      "| fps                | 3193      |\n",
      "| nupdates           | 88        |\n",
      "| policy_entropy     | 0.585     |\n",
      "| policy_loss        | -0.168    |\n",
      "| total_timesteps    | 17977767  |\n",
      "| value_loss         | 12.1      |\n",
      "----------------------------------\n",
      "18389088 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.59\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 220.5936 |\n",
      "| explained_variance | 0.865    |\n",
      "| fps                | 3193     |\n",
      "| nupdates           | 89       |\n",
      "| policy_entropy     | 0.574    |\n",
      "| policy_loss        | -0.093   |\n",
      "| total_timesteps    | 18184408 |\n",
      "| value_loss         | 10.6     |\n",
      "---------------------------------\n",
      "18596520 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.79\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 223.6306 |\n",
      "| explained_variance | 0.86     |\n",
      "| fps                | 3193     |\n",
      "| nupdates           | 90       |\n",
      "| policy_entropy     | 0.582    |\n",
      "| policy_loss        | 0.000438 |\n",
      "| total_timesteps    | 18391049 |\n",
      "| value_loss         | 11.3     |\n",
      "---------------------------------\n",
      "18803952 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.59\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 213.73526 |\n",
      "| explained_variance | 0.851     |\n",
      "| fps                | 3193      |\n",
      "| nupdates           | 91        |\n",
      "| policy_entropy     | 0.575     |\n",
      "| policy_loss        | 0.0119    |\n",
      "| total_timesteps    | 18597690  |\n",
      "| value_loss         | 12.1      |\n",
      "----------------------------------\n",
      "19008972 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.69\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 218.8904 |\n",
      "| explained_variance | 0.865    |\n",
      "| fps                | 3194     |\n",
      "| nupdates           | 92       |\n",
      "| policy_entropy     | 0.584    |\n",
      "| policy_loss        | 0.159    |\n",
      "| total_timesteps    | 18804331 |\n",
      "| value_loss         | 10.4     |\n",
      "---------------------------------\n",
      "19216404 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.10\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 217.78055 |\n",
      "| explained_variance | 0.86      |\n",
      "| fps                | 3194      |\n",
      "| nupdates           | 93        |\n",
      "| policy_entropy     | 0.571     |\n",
      "| policy_loss        | 0.207     |\n",
      "| total_timesteps    | 19010972  |\n",
      "| value_loss         | 10.8      |\n",
      "----------------------------------\n",
      "19423836 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.09\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.00684 |\n",
      "| explained_variance | 0.855     |\n",
      "| fps                | 3194      |\n",
      "| nupdates           | 94        |\n",
      "| policy_entropy     | 0.571     |\n",
      "| policy_loss        | 0.208     |\n",
      "| total_timesteps    | 19217613  |\n",
      "| value_loss         | 11.6      |\n",
      "----------------------------------\n",
      "19628856 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.76\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 222.898  |\n",
      "| explained_variance | 0.843    |\n",
      "| fps                | 3194     |\n",
      "| nupdates           | 95       |\n",
      "| policy_entropy     | 0.567    |\n",
      "| policy_loss        | 0.24     |\n",
      "| total_timesteps    | 19424254 |\n",
      "| value_loss         | 11.9     |\n",
      "---------------------------------\n",
      "19836288 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.76\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 216.57071 |\n",
      "| explained_variance | 0.857     |\n",
      "| fps                | 3194      |\n",
      "| nupdates           | 96        |\n",
      "| policy_entropy     | 0.564     |\n",
      "| policy_loss        | 0.214     |\n",
      "| total_timesteps    | 19630895  |\n",
      "| value_loss         | 11.6      |\n",
      "----------------------------------\n",
      "20043720 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.70\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.28642 |\n",
      "| explained_variance | 0.85      |\n",
      "| fps                | 3194      |\n",
      "| nupdates           | 97        |\n",
      "| policy_entropy     | 0.567     |\n",
      "| policy_loss        | 0.246     |\n",
      "| total_timesteps    | 19837536  |\n",
      "| value_loss         | 11.8      |\n",
      "----------------------------------\n",
      "20248740 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.32\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 222.04393 |\n",
      "| explained_variance | 0.853     |\n",
      "| fps                | 3194      |\n",
      "| nupdates           | 98        |\n",
      "| policy_entropy     | 0.566     |\n",
      "| policy_loss        | 0.198     |\n",
      "| total_timesteps    | 20044177  |\n",
      "| value_loss         | 11.5      |\n",
      "----------------------------------\n",
      "20456172 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.48\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.55211 |\n",
      "| explained_variance | 0.863     |\n",
      "| fps                | 3194      |\n",
      "| nupdates           | 99        |\n",
      "| policy_entropy     | 0.564     |\n",
      "| policy_loss        | 0.206     |\n",
      "| total_timesteps    | 20250818  |\n",
      "| value_loss         | 10.7      |\n",
      "----------------------------------\n",
      "20663604 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.81\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 217.30104 |\n",
      "| explained_variance | 0.853     |\n",
      "| fps                | 3194      |\n",
      "| nupdates           | 100       |\n",
      "| policy_entropy     | 0.561     |\n",
      "| policy_loss        | 0.122     |\n",
      "| total_timesteps    | 20457459  |\n",
      "| value_loss         | 11.5      |\n",
      "----------------------------------\n",
      "20868624 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.53\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.35867 |\n",
      "| explained_variance | 0.852     |\n",
      "| fps                | 3194      |\n",
      "| nupdates           | 101       |\n",
      "| policy_entropy     | 0.561     |\n",
      "| policy_loss        | 0.0685    |\n",
      "| total_timesteps    | 20664100  |\n",
      "| value_loss         | 11.5      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21076056 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.57\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 224.86424 |\n",
      "| explained_variance | 0.872     |\n",
      "| fps                | 3194      |\n",
      "| nupdates           | 102       |\n",
      "| policy_entropy     | 0.567     |\n",
      "| policy_loss        | 0.0534    |\n",
      "| total_timesteps    | 20870741  |\n",
      "| value_loss         | 10        |\n",
      "----------------------------------\n",
      "21283488 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.27\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.77673 |\n",
      "| explained_variance | 0.855     |\n",
      "| fps                | 3194      |\n",
      "| nupdates           | 103       |\n",
      "| policy_entropy     | 0.559     |\n",
      "| policy_loss        | -0.0537   |\n",
      "| total_timesteps    | 21077382  |\n",
      "| value_loss         | 11.1      |\n",
      "----------------------------------\n",
      "21488508 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.91\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.80359 |\n",
      "| explained_variance | 0.856     |\n",
      "| fps                | 3194      |\n",
      "| nupdates           | 104       |\n",
      "| policy_entropy     | 0.564     |\n",
      "| policy_loss        | -0.105    |\n",
      "| total_timesteps    | 21284023  |\n",
      "| value_loss         | 10.9      |\n",
      "----------------------------------\n",
      "21695940 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.88\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 219.1886 |\n",
      "| explained_variance | 0.862    |\n",
      "| fps                | 3194     |\n",
      "| nupdates           | 105      |\n",
      "| policy_entropy     | 0.568    |\n",
      "| policy_loss        | -0.168   |\n",
      "| total_timesteps    | 21490664 |\n",
      "| value_loss         | 11.1     |\n",
      "---------------------------------\n",
      "21903372 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 221.11\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 223.4267 |\n",
      "| explained_variance | 0.849    |\n",
      "| fps                | 3194     |\n",
      "| nupdates           | 106      |\n",
      "| policy_entropy     | 0.555    |\n",
      "| policy_loss        | -0.156   |\n",
      "| total_timesteps    | 21697305 |\n",
      "| value_loss         | 10.9     |\n",
      "---------------------------------\n",
      "22108392 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.84\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 222.46637 |\n",
      "| explained_variance | 0.865     |\n",
      "| fps                | 3194      |\n",
      "| nupdates           | 107       |\n",
      "| policy_entropy     | 0.564     |\n",
      "| policy_loss        | -0.231    |\n",
      "| total_timesteps    | 21903946  |\n",
      "| value_loss         | 10.7      |\n",
      "----------------------------------\n",
      "22315824 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.38\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.43492 |\n",
      "| explained_variance | 0.846     |\n",
      "| fps                | 3194      |\n",
      "| nupdates           | 108       |\n",
      "| policy_entropy     | 0.554     |\n",
      "| policy_loss        | -0.22     |\n",
      "| total_timesteps    | 22110587  |\n",
      "| value_loss         | 11.5      |\n",
      "----------------------------------\n",
      "22523256 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.47\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 226.02792 |\n",
      "| explained_variance | 0.849     |\n",
      "| fps                | 3194      |\n",
      "| nupdates           | 109       |\n",
      "| policy_entropy     | 0.555     |\n",
      "| policy_loss        | -0.247    |\n",
      "| total_timesteps    | 22317228  |\n",
      "| value_loss         | 11.8      |\n",
      "----------------------------------\n",
      "22728276 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.98\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 218.8149 |\n",
      "| explained_variance | 0.842    |\n",
      "| fps                | 3194     |\n",
      "| nupdates           | 110      |\n",
      "| policy_entropy     | 0.56     |\n",
      "| policy_loss        | -0.242   |\n",
      "| total_timesteps    | 22523869 |\n",
      "| value_loss         | 12.6     |\n",
      "---------------------------------\n",
      "22935708 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.86\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.35472 |\n",
      "| explained_variance | 0.834     |\n",
      "| fps                | 3194      |\n",
      "| nupdates           | 111       |\n",
      "| policy_entropy     | 0.555     |\n",
      "| policy_loss        | -0.233    |\n",
      "| total_timesteps    | 22730510  |\n",
      "| value_loss         | 12.6      |\n",
      "----------------------------------\n",
      "23143140 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.55\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 216.49094 |\n",
      "| explained_variance | 0.854     |\n",
      "| fps                | 3194      |\n",
      "| nupdates           | 112       |\n",
      "| policy_entropy     | 0.562     |\n",
      "| policy_loss        | -0.217    |\n",
      "| total_timesteps    | 22937151  |\n",
      "| value_loss         | 11.8      |\n",
      "----------------------------------\n",
      "23348160 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.84\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.90527 |\n",
      "| explained_variance | 0.851     |\n",
      "| fps                | 3194      |\n",
      "| nupdates           | 113       |\n",
      "| policy_entropy     | 0.556     |\n",
      "| policy_loss        | -0.197    |\n",
      "| total_timesteps    | 23143792  |\n",
      "| value_loss         | 12.3      |\n",
      "----------------------------------\n",
      "23555592 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.13\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.84694 |\n",
      "| explained_variance | 0.867     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 114       |\n",
      "| policy_entropy     | 0.555     |\n",
      "| policy_loss        | -0.111    |\n",
      "| total_timesteps    | 23350433  |\n",
      "| value_loss         | 10.5      |\n",
      "----------------------------------\n",
      "23763024 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.59\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 218.1462 |\n",
      "| explained_variance | 0.857    |\n",
      "| fps                | 3195     |\n",
      "| nupdates           | 115      |\n",
      "| policy_entropy     | 0.554    |\n",
      "| policy_loss        | -0.035   |\n",
      "| total_timesteps    | 23557074 |\n",
      "| value_loss         | 11.4     |\n",
      "---------------------------------\n",
      "23968044 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.66\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 223.7041 |\n",
      "| explained_variance | 0.851    |\n",
      "| fps                | 3195     |\n",
      "| nupdates           | 116      |\n",
      "| policy_entropy     | 0.553    |\n",
      "| policy_loss        | -0.00192 |\n",
      "| total_timesteps    | 23763715 |\n",
      "| value_loss         | 11.8     |\n",
      "---------------------------------\n",
      "24175476 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.84\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.85904 |\n",
      "| explained_variance | 0.853     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 117       |\n",
      "| policy_entropy     | 0.55      |\n",
      "| policy_loss        | 0.068     |\n",
      "| total_timesteps    | 23970356  |\n",
      "| value_loss         | 11.2      |\n",
      "----------------------------------\n",
      "24382908 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.50\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 221.2607 |\n",
      "| explained_variance | 0.853    |\n",
      "| fps                | 3195     |\n",
      "| nupdates           | 118      |\n",
      "| policy_entropy     | 0.55     |\n",
      "| policy_loss        | 0.141    |\n",
      "| total_timesteps    | 24176997 |\n",
      "| value_loss         | 11.4     |\n",
      "---------------------------------\n",
      "24587928 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.37\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 217.39919 |\n",
      "| explained_variance | 0.844     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 119       |\n",
      "| policy_entropy     | 0.551     |\n",
      "| policy_loss        | 0.0928    |\n",
      "| total_timesteps    | 24383638  |\n",
      "| value_loss         | 12.7      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24795360 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.90\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.96861 |\n",
      "| explained_variance | 0.854     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 120       |\n",
      "| policy_entropy     | 0.541     |\n",
      "| policy_loss        | 0.164     |\n",
      "| total_timesteps    | 24590279  |\n",
      "| value_loss         | 11.4      |\n",
      "----------------------------------\n",
      "25002792 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.24\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.21857 |\n",
      "| explained_variance | 0.863     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 121       |\n",
      "| policy_entropy     | 0.551     |\n",
      "| policy_loss        | 0.144     |\n",
      "| total_timesteps    | 24796920  |\n",
      "| value_loss         | 11.4      |\n",
      "----------------------------------\n",
      "25207812 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.08\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.01309 |\n",
      "| explained_variance | 0.863     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 122       |\n",
      "| policy_entropy     | 0.549     |\n",
      "| policy_loss        | 0.166     |\n",
      "| total_timesteps    | 25003561  |\n",
      "| value_loss         | 11.2      |\n",
      "----------------------------------\n",
      "25415244 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.66\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.76115 |\n",
      "| explained_variance | 0.842     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 123       |\n",
      "| policy_entropy     | 0.542     |\n",
      "| policy_loss        | 0.113     |\n",
      "| total_timesteps    | 25210202  |\n",
      "| value_loss         | 12.9      |\n",
      "----------------------------------\n",
      "25622676 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 221.83\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.59216 |\n",
      "| explained_variance | 0.846     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 124       |\n",
      "| policy_entropy     | 0.52      |\n",
      "| policy_loss        | 0.184     |\n",
      "| total_timesteps    | 25416843  |\n",
      "| value_loss         | 11.3      |\n",
      "----------------------------------\n",
      "25827696 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.60\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 219.3331 |\n",
      "| explained_variance | 0.85     |\n",
      "| fps                | 3195     |\n",
      "| nupdates           | 125      |\n",
      "| policy_entropy     | 0.532    |\n",
      "| policy_loss        | 0.0786   |\n",
      "| total_timesteps    | 25623484 |\n",
      "| value_loss         | 12       |\n",
      "---------------------------------\n",
      "26035128 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 221.00\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 224.20256 |\n",
      "| explained_variance | 0.855     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 126       |\n",
      "| policy_entropy     | 0.527     |\n",
      "| policy_loss        | 0.0904    |\n",
      "| total_timesteps    | 25830125  |\n",
      "| value_loss         | 11        |\n",
      "----------------------------------\n",
      "26242560 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.58\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 217.76114 |\n",
      "| explained_variance | 0.85      |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 127       |\n",
      "| policy_entropy     | 0.53      |\n",
      "| policy_loss        | 0.0216    |\n",
      "| total_timesteps    | 26036766  |\n",
      "| value_loss         | 11.6      |\n",
      "----------------------------------\n",
      "26447580 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.01\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 215.23592 |\n",
      "| explained_variance | 0.855     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 128       |\n",
      "| policy_entropy     | 0.532     |\n",
      "| policy_loss        | -0.0553   |\n",
      "| total_timesteps    | 26243407  |\n",
      "| value_loss         | 11.6      |\n",
      "----------------------------------\n",
      "26655012 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 221.59\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 222.70433 |\n",
      "| explained_variance | 0.857     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 129       |\n",
      "| policy_entropy     | 0.519     |\n",
      "| policy_loss        | -0.055    |\n",
      "| total_timesteps    | 26450048  |\n",
      "| value_loss         | 10.7      |\n",
      "----------------------------------\n",
      "26862444 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.93\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 224.45499 |\n",
      "| explained_variance | 0.839     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 130       |\n",
      "| policy_entropy     | 0.521     |\n",
      "| policy_loss        | -0.126    |\n",
      "| total_timesteps    | 26656689  |\n",
      "| value_loss         | 12.5      |\n",
      "----------------------------------\n",
      "27067464 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.91\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.25664 |\n",
      "| explained_variance | 0.866     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 131       |\n",
      "| policy_entropy     | 0.526     |\n",
      "| policy_loss        | -0.11     |\n",
      "| total_timesteps    | 26863330  |\n",
      "| value_loss         | 10.3      |\n",
      "----------------------------------\n",
      "27274896 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.77\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 219.4924 |\n",
      "| explained_variance | 0.859    |\n",
      "| fps                | 3196     |\n",
      "| nupdates           | 132      |\n",
      "| policy_entropy     | 0.533    |\n",
      "| policy_loss        | -0.208   |\n",
      "| total_timesteps    | 27069971 |\n",
      "| value_loss         | 11.8     |\n",
      "---------------------------------\n",
      "27482328 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.23\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.27788 |\n",
      "| explained_variance | 0.856     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 133       |\n",
      "| policy_entropy     | 0.522     |\n",
      "| policy_loss        | -0.178    |\n",
      "| total_timesteps    | 27276612  |\n",
      "| value_loss         | 11.6      |\n",
      "----------------------------------\n",
      "27689760 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.22\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 221.474  |\n",
      "| explained_variance | 0.842    |\n",
      "| fps                | 3195     |\n",
      "| nupdates           | 134      |\n",
      "| policy_entropy     | 0.527    |\n",
      "| policy_loss        | -0.223   |\n",
      "| total_timesteps    | 27483253 |\n",
      "| value_loss         | 13.1     |\n",
      "---------------------------------\n",
      "27894780 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.84\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 216.31635 |\n",
      "| explained_variance | 0.848     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 135       |\n",
      "| policy_entropy     | 0.523     |\n",
      "| policy_loss        | -0.174    |\n",
      "| total_timesteps    | 27689894  |\n",
      "| value_loss         | 11.9      |\n",
      "----------------------------------\n",
      "28102212 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.73\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.81949 |\n",
      "| explained_variance | 0.856     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 136       |\n",
      "| policy_entropy     | 0.527     |\n",
      "| policy_loss        | -0.155    |\n",
      "| total_timesteps    | 27896535  |\n",
      "| value_loss         | 11.6      |\n",
      "----------------------------------\n",
      "28309644 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.48\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.58719 |\n",
      "| explained_variance | 0.857     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 137       |\n",
      "| policy_entropy     | 0.525     |\n",
      "| policy_loss        | -0.123    |\n",
      "| total_timesteps    | 28103176  |\n",
      "| value_loss         | 11.4      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28514664 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.82\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.66461 |\n",
      "| explained_variance | 0.85      |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 138       |\n",
      "| policy_entropy     | 0.517     |\n",
      "| policy_loss        | -0.0529   |\n",
      "| total_timesteps    | 28309817  |\n",
      "| value_loss         | 11.3      |\n",
      "----------------------------------\n",
      "28722096 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.54\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 217.74841 |\n",
      "| explained_variance | 0.86      |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 139       |\n",
      "| policy_entropy     | 0.517     |\n",
      "| policy_loss        | -0.0296   |\n",
      "| total_timesteps    | 28516458  |\n",
      "| value_loss         | 11.2      |\n",
      "----------------------------------\n",
      "28929528 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.78\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.14737 |\n",
      "| explained_variance | 0.854     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 140       |\n",
      "| policy_entropy     | 0.522     |\n",
      "| policy_loss        | -0.0226   |\n",
      "| total_timesteps    | 28723099  |\n",
      "| value_loss         | 11.9      |\n",
      "----------------------------------\n",
      "29134548 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.17\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 213.33511 |\n",
      "| explained_variance | 0.85      |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 141       |\n",
      "| policy_entropy     | 0.523     |\n",
      "| policy_loss        | 0.00898   |\n",
      "| total_timesteps    | 28929740  |\n",
      "| value_loss         | 12        |\n",
      "----------------------------------\n",
      "29341980 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.31\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 218.2896 |\n",
      "| explained_variance | 0.852    |\n",
      "| fps                | 3195     |\n",
      "| nupdates           | 142      |\n",
      "| policy_entropy     | 0.514    |\n",
      "| policy_loss        | 0.047    |\n",
      "| total_timesteps    | 29136381 |\n",
      "| value_loss         | 11.6     |\n",
      "---------------------------------\n",
      "29549412 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.82\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 224.62544 |\n",
      "| explained_variance | 0.844     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 143       |\n",
      "| policy_entropy     | 0.52      |\n",
      "| policy_loss        | 0.0542    |\n",
      "| total_timesteps    | 29343022  |\n",
      "| value_loss         | 12.7      |\n",
      "----------------------------------\n",
      "29754432 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.16\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.77846 |\n",
      "| explained_variance | 0.842     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 144       |\n",
      "| policy_entropy     | 0.508     |\n",
      "| policy_loss        | 0.108     |\n",
      "| total_timesteps    | 29549663  |\n",
      "| value_loss         | 12.4      |\n",
      "----------------------------------\n",
      "29961864 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.82\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.44025 |\n",
      "| explained_variance | 0.841     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 145       |\n",
      "| policy_entropy     | 0.506     |\n",
      "| policy_loss        | 0.133     |\n",
      "| total_timesteps    | 29756304  |\n",
      "| value_loss         | 12.5      |\n",
      "----------------------------------\n",
      "30169296 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.55\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 216.91492 |\n",
      "| explained_variance | 0.863     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 146       |\n",
      "| policy_entropy     | 0.511     |\n",
      "| policy_loss        | 0.134     |\n",
      "| total_timesteps    | 29962945  |\n",
      "| value_loss         | 11.1      |\n",
      "----------------------------------\n",
      "30374316 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.96\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.37234 |\n",
      "| explained_variance | 0.853     |\n",
      "| fps                | 3195      |\n",
      "| nupdates           | 147       |\n",
      "| policy_entropy     | 0.515     |\n",
      "| policy_loss        | 0.0716    |\n",
      "| total_timesteps    | 30169586  |\n",
      "| value_loss         | 11.9      |\n",
      "----------------------------------\n",
      "30581748 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.38\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 213.36284 |\n",
      "| explained_variance | 0.862     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 148       |\n",
      "| policy_entropy     | 0.51      |\n",
      "| policy_loss        | 0.134     |\n",
      "| total_timesteps    | 30376227  |\n",
      "| value_loss         | 10.8      |\n",
      "----------------------------------\n",
      "30789180 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.61\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.16176 |\n",
      "| explained_variance | 0.849     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 149       |\n",
      "| policy_entropy     | 0.513     |\n",
      "| policy_loss        | 0.0615    |\n",
      "| total_timesteps    | 30582868  |\n",
      "| value_loss         | 11.9      |\n",
      "----------------------------------\n",
      "30994200 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.17\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 220.6186 |\n",
      "| explained_variance | 0.85     |\n",
      "| fps                | 3196     |\n",
      "| nupdates           | 150      |\n",
      "| policy_entropy     | 0.506    |\n",
      "| policy_loss        | 0.0579   |\n",
      "| total_timesteps    | 30789509 |\n",
      "| value_loss         | 11.7     |\n",
      "---------------------------------\n",
      "31201632 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.74\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.47456 |\n",
      "| explained_variance | 0.861     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 151       |\n",
      "| policy_entropy     | 0.504     |\n",
      "| policy_loss        | 0.0468    |\n",
      "| total_timesteps    | 30996150  |\n",
      "| value_loss         | 10.6      |\n",
      "----------------------------------\n",
      "31409064 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.21\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.98807 |\n",
      "| explained_variance | 0.85      |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 152       |\n",
      "| policy_entropy     | 0.498     |\n",
      "| policy_loss        | -0.0191   |\n",
      "| total_timesteps    | 31202791  |\n",
      "| value_loss         | 11.7      |\n",
      "----------------------------------\n",
      "31614084 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.74\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 217.86531 |\n",
      "| explained_variance | 0.851     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 153       |\n",
      "| policy_entropy     | 0.498     |\n",
      "| policy_loss        | -0.0417   |\n",
      "| total_timesteps    | 31409432  |\n",
      "| value_loss         | 11.7      |\n",
      "----------------------------------\n",
      "31821516 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.51\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.17885 |\n",
      "| explained_variance | 0.846     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 154       |\n",
      "| policy_entropy     | 0.501     |\n",
      "| policy_loss        | -0.0595   |\n",
      "| total_timesteps    | 31616073  |\n",
      "| value_loss         | 12        |\n",
      "----------------------------------\n",
      "32028948 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.46\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.98254 |\n",
      "| explained_variance | 0.835     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 155       |\n",
      "| policy_entropy     | 0.488     |\n",
      "| policy_loss        | -0.123    |\n",
      "| total_timesteps    | 31822714  |\n",
      "| value_loss         | 12.8      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32233968 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.23\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 219.9899 |\n",
      "| explained_variance | 0.859    |\n",
      "| fps                | 3196     |\n",
      "| nupdates           | 156      |\n",
      "| policy_entropy     | 0.491    |\n",
      "| policy_loss        | -0.112   |\n",
      "| total_timesteps    | 32029355 |\n",
      "| value_loss         | 11.2     |\n",
      "---------------------------------\n",
      "32441400 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.41\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 214.99214 |\n",
      "| explained_variance | 0.828     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 157       |\n",
      "| policy_entropy     | 0.488     |\n",
      "| policy_loss        | -0.116    |\n",
      "| total_timesteps    | 32235996  |\n",
      "| value_loss         | 13.2      |\n",
      "----------------------------------\n",
      "32648832 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.31\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 222.67775 |\n",
      "| explained_variance | 0.858     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 158       |\n",
      "| policy_entropy     | 0.492     |\n",
      "| policy_loss        | -0.128    |\n",
      "| total_timesteps    | 32442637  |\n",
      "| value_loss         | 11.4      |\n",
      "----------------------------------\n",
      "32853852 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.12\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 214.94096 |\n",
      "| explained_variance | 0.84      |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 159       |\n",
      "| policy_entropy     | 0.493     |\n",
      "| policy_loss        | -0.131    |\n",
      "| total_timesteps    | 32649278  |\n",
      "| value_loss         | 12.9      |\n",
      "----------------------------------\n",
      "33061284 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.00\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.23521 |\n",
      "| explained_variance | 0.864     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 160       |\n",
      "| policy_entropy     | 0.504     |\n",
      "| policy_loss        | -0.0686   |\n",
      "| total_timesteps    | 32855919  |\n",
      "| value_loss         | 11.1      |\n",
      "----------------------------------\n",
      "33268716 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.22\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.49756 |\n",
      "| explained_variance | 0.865     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 161       |\n",
      "| policy_entropy     | 0.502     |\n",
      "| policy_loss        | -0.0514   |\n",
      "| total_timesteps    | 33062560  |\n",
      "| value_loss         | 10.9      |\n",
      "----------------------------------\n",
      "33473736 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.37\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 222.9734 |\n",
      "| explained_variance | 0.85     |\n",
      "| fps                | 3196     |\n",
      "| nupdates           | 162      |\n",
      "| policy_entropy     | 0.49     |\n",
      "| policy_loss        | -0.0311  |\n",
      "| total_timesteps    | 33269201 |\n",
      "| value_loss         | 11.8     |\n",
      "---------------------------------\n",
      "33681168 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.21\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 215.63849 |\n",
      "| explained_variance | 0.859     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 163       |\n",
      "| policy_entropy     | 0.493     |\n",
      "| policy_loss        | -0.00033  |\n",
      "| total_timesteps    | 33475842  |\n",
      "| value_loss         | 11.4      |\n",
      "----------------------------------\n",
      "33888600 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.99\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 223.24947 |\n",
      "| explained_variance | 0.863     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 164       |\n",
      "| policy_entropy     | 0.491     |\n",
      "| policy_loss        | 0.0758    |\n",
      "| total_timesteps    | 33682483  |\n",
      "| value_loss         | 10.2      |\n",
      "----------------------------------\n",
      "34093620 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.69\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 216.27199 |\n",
      "| explained_variance | 0.849     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 165       |\n",
      "| policy_entropy     | 0.493     |\n",
      "| policy_loss        | 0.0217    |\n",
      "| total_timesteps    | 33889124  |\n",
      "| value_loss         | 12        |\n",
      "----------------------------------\n",
      "34301052 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.94\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 222.93839 |\n",
      "| explained_variance | 0.852     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 166       |\n",
      "| policy_entropy     | 0.481     |\n",
      "| policy_loss        | 0.0503    |\n",
      "| total_timesteps    | 34095765  |\n",
      "| value_loss         | 11.7      |\n",
      "----------------------------------\n",
      "34508484 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 218.95\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 217.7059 |\n",
      "| explained_variance | 0.859    |\n",
      "| fps                | 3196     |\n",
      "| nupdates           | 167      |\n",
      "| policy_entropy     | 0.5      |\n",
      "| policy_loss        | 0.02     |\n",
      "| total_timesteps    | 34302406 |\n",
      "| value_loss         | 11.6     |\n",
      "---------------------------------\n",
      "34713504 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 221.36\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.14462 |\n",
      "| explained_variance | 0.856     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 168       |\n",
      "| policy_entropy     | 0.478     |\n",
      "| policy_loss        | 0.0635    |\n",
      "| total_timesteps    | 34509047  |\n",
      "| value_loss         | 10.7      |\n",
      "----------------------------------\n",
      "34920936 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.35\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 217.49826 |\n",
      "| explained_variance | 0.854     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 169       |\n",
      "| policy_entropy     | 0.479     |\n",
      "| policy_loss        | 0.0446    |\n",
      "| total_timesteps    | 34715688  |\n",
      "| value_loss         | 11.4      |\n",
      "----------------------------------\n",
      "35128368 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 220.13\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.87222 |\n",
      "| explained_variance | 0.855     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 170       |\n",
      "| policy_entropy     | 0.489     |\n",
      "| policy_loss        | 0.0131    |\n",
      "| total_timesteps    | 34922329  |\n",
      "| value_loss         | 11.3      |\n",
      "----------------------------------\n",
      "35333388 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.66\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 222.73242 |\n",
      "| explained_variance | 0.861     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 171       |\n",
      "| policy_entropy     | 0.496     |\n",
      "| policy_loss        | -0.0227   |\n",
      "| total_timesteps    | 35128970  |\n",
      "| value_loss         | 11.3      |\n",
      "----------------------------------\n",
      "35540820 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 219.76\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.38136 |\n",
      "| explained_variance | 0.853     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 172       |\n",
      "| policy_entropy     | 0.494     |\n",
      "| policy_loss        | -0.00664  |\n",
      "| total_timesteps    | 35335611  |\n",
      "| value_loss         | 11.5      |\n",
      "----------------------------------\n",
      "35748252 timesteps\n",
      "Best mean reward: 221.93 - Last mean reward per episode: 221.00\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 226.13957 |\n",
      "| explained_variance | 0.861     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 173       |\n",
      "| policy_entropy     | 0.487     |\n",
      "| policy_loss        | 0.00561   |\n",
      "| total_timesteps    | 35542252  |\n",
      "| value_loss         | 10.7      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35953272 timesteps\n",
      "New best mean reward: 222.05 - Last best reward per episode: 221.93\n",
      "saved as  saved/best_perus\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.58725 |\n",
      "| explained_variance | 0.858     |\n",
      "| fps                | 3196      |\n",
      "| nupdates           | 174       |\n",
      "| policy_entropy     | 0.482     |\n",
      "| policy_loss        | 0.0151    |\n",
      "| total_timesteps    | 35748893  |\n",
      "| value_loss         | 10.5      |\n",
      "----------------------------------\n",
      "36160704 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.08\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 217.64244 |\n",
      "| explained_variance | 0.855     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 175       |\n",
      "| policy_entropy     | 0.502     |\n",
      "| policy_loss        | -0.0474   |\n",
      "| total_timesteps    | 35955534  |\n",
      "| value_loss         | 11.6      |\n",
      "----------------------------------\n",
      "36368136 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.06\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.56538 |\n",
      "| explained_variance | 0.85      |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 176       |\n",
      "| policy_entropy     | 0.496     |\n",
      "| policy_loss        | -0.125    |\n",
      "| total_timesteps    | 36162175  |\n",
      "| value_loss         | 12.8      |\n",
      "----------------------------------\n",
      "36573156 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.14\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.88991 |\n",
      "| explained_variance | 0.855     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 177       |\n",
      "| policy_entropy     | 0.496     |\n",
      "| policy_loss        | -0.028    |\n",
      "| total_timesteps    | 36368816  |\n",
      "| value_loss         | 11.2      |\n",
      "----------------------------------\n",
      "36780588 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.09\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 217.5772 |\n",
      "| explained_variance | 0.851    |\n",
      "| fps                | 3197     |\n",
      "| nupdates           | 178      |\n",
      "| policy_entropy     | 0.5      |\n",
      "| policy_loss        | -0.0661  |\n",
      "| total_timesteps    | 36575457 |\n",
      "| value_loss         | 12       |\n",
      "---------------------------------\n",
      "36988020 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 218.43\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.17404 |\n",
      "| explained_variance | 0.857     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 179       |\n",
      "| policy_entropy     | 0.51      |\n",
      "| policy_loss        | -0.0364   |\n",
      "| total_timesteps    | 36782098  |\n",
      "| value_loss         | 11.7      |\n",
      "----------------------------------\n",
      "37193040 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.44\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 223.1915 |\n",
      "| explained_variance | 0.85     |\n",
      "| fps                | 3197     |\n",
      "| nupdates           | 180      |\n",
      "| policy_entropy     | 0.494    |\n",
      "| policy_loss        | -0.0356  |\n",
      "| total_timesteps    | 36988739 |\n",
      "| value_loss         | 11.8     |\n",
      "---------------------------------\n",
      "37400472 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 221.30\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.47168 |\n",
      "| explained_variance | 0.848     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 181       |\n",
      "| policy_entropy     | 0.49      |\n",
      "| policy_loss        | 0.0248    |\n",
      "| total_timesteps    | 37195380  |\n",
      "| value_loss         | 11.3      |\n",
      "----------------------------------\n",
      "37607904 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.38\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.09259 |\n",
      "| explained_variance | 0.857     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 182       |\n",
      "| policy_entropy     | 0.504     |\n",
      "| policy_loss        | -0.0253   |\n",
      "| total_timesteps    | 37402021  |\n",
      "| value_loss         | 11.4      |\n",
      "----------------------------------\n",
      "37812924 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.09\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 217.78113 |\n",
      "| explained_variance | 0.842     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 183       |\n",
      "| policy_entropy     | 0.494     |\n",
      "| policy_loss        | -0.0141   |\n",
      "| total_timesteps    | 37608662  |\n",
      "| value_loss         | 12.3      |\n",
      "----------------------------------\n",
      "38020356 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.81\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 222.49698 |\n",
      "| explained_variance | 0.862     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 184       |\n",
      "| policy_entropy     | 0.504     |\n",
      "| policy_loss        | 0.0251    |\n",
      "| total_timesteps    | 37815303  |\n",
      "| value_loss         | 10.7      |\n",
      "----------------------------------\n",
      "38227788 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.59\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.06067 |\n",
      "| explained_variance | 0.855     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 185       |\n",
      "| policy_entropy     | 0.495     |\n",
      "| policy_loss        | 0.0151    |\n",
      "| total_timesteps    | 38021944  |\n",
      "| value_loss         | 11.4      |\n",
      "----------------------------------\n",
      "38432808 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 221.20\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.86093 |\n",
      "| explained_variance | 0.842     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 186       |\n",
      "| policy_entropy     | 0.487     |\n",
      "| policy_loss        | -0.00599  |\n",
      "| total_timesteps    | 38228585  |\n",
      "| value_loss         | 12        |\n",
      "----------------------------------\n",
      "38640240 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.68\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 223.13937 |\n",
      "| explained_variance | 0.863     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 187       |\n",
      "| policy_entropy     | 0.498     |\n",
      "| policy_loss        | 0.0101    |\n",
      "| total_timesteps    | 38435226  |\n",
      "| value_loss         | 10.8      |\n",
      "----------------------------------\n",
      "38847672 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.21\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 214.28609 |\n",
      "| explained_variance | 0.863     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 188       |\n",
      "| policy_entropy     | 0.505     |\n",
      "| policy_loss        | 0.000542  |\n",
      "| total_timesteps    | 38641867  |\n",
      "| value_loss         | 10.9      |\n",
      "----------------------------------\n",
      "39052692 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.86\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.83595 |\n",
      "| explained_variance | 0.867     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 189       |\n",
      "| policy_entropy     | 0.504     |\n",
      "| policy_loss        | -0.0413   |\n",
      "| total_timesteps    | 38848508  |\n",
      "| value_loss         | 11.1      |\n",
      "----------------------------------\n",
      "39260124 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 221.54\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 223.17873 |\n",
      "| explained_variance | 0.854     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 190       |\n",
      "| policy_entropy     | 0.491     |\n",
      "| policy_loss        | 0.0104    |\n",
      "| total_timesteps    | 39055149  |\n",
      "| value_loss         | 10.9      |\n",
      "----------------------------------\n",
      "39467556 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.02\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.33524 |\n",
      "| explained_variance | 0.857     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 191       |\n",
      "| policy_entropy     | 0.505     |\n",
      "| policy_loss        | -0.0499   |\n",
      "| total_timesteps    | 39261790  |\n",
      "| value_loss         | 12        |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39672576 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.95\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 220.1174 |\n",
      "| explained_variance | 0.849    |\n",
      "| fps                | 3197     |\n",
      "| nupdates           | 192      |\n",
      "| policy_entropy     | 0.49     |\n",
      "| policy_loss        | -0.0161  |\n",
      "| total_timesteps    | 39468431 |\n",
      "| value_loss         | 11.7     |\n",
      "---------------------------------\n",
      "39880008 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.74\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 224.49164 |\n",
      "| explained_variance | 0.854     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 193       |\n",
      "| policy_entropy     | 0.491     |\n",
      "| policy_loss        | -0.0272   |\n",
      "| total_timesteps    | 39675072  |\n",
      "| value_loss         | 11.8      |\n",
      "----------------------------------\n",
      "40087440 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 218.95\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 215.165  |\n",
      "| explained_variance | 0.865    |\n",
      "| fps                | 3197     |\n",
      "| nupdates           | 194      |\n",
      "| policy_entropy     | 0.5      |\n",
      "| policy_loss        | -0.0439  |\n",
      "| total_timesteps    | 39881713 |\n",
      "| value_loss         | 11.2     |\n",
      "---------------------------------\n",
      "40292460 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.14\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 226.7963 |\n",
      "| explained_variance | 0.84     |\n",
      "| fps                | 3197     |\n",
      "| nupdates           | 195      |\n",
      "| policy_entropy     | 0.485    |\n",
      "| policy_loss        | -0.031   |\n",
      "| total_timesteps    | 40088354 |\n",
      "| value_loss         | 12.8     |\n",
      "---------------------------------\n",
      "40499892 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.49\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 217.64731 |\n",
      "| explained_variance | 0.843     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 196       |\n",
      "| policy_entropy     | 0.492     |\n",
      "| policy_loss        | -0.0545   |\n",
      "| total_timesteps    | 40294995  |\n",
      "| value_loss         | 12.5      |\n",
      "----------------------------------\n",
      "40707324 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.18\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 216.84697 |\n",
      "| explained_variance | 0.848     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 197       |\n",
      "| policy_entropy     | 0.49      |\n",
      "| policy_loss        | -0.044    |\n",
      "| total_timesteps    | 40501636  |\n",
      "| value_loss         | 12.8      |\n",
      "----------------------------------\n",
      "40912344 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.91\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.31862 |\n",
      "| explained_variance | 0.865     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 198       |\n",
      "| policy_entropy     | 0.493     |\n",
      "| policy_loss        | 0.0321    |\n",
      "| total_timesteps    | 40708277  |\n",
      "| value_loss         | 10.6      |\n",
      "----------------------------------\n",
      "41119776 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.03\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.63028 |\n",
      "| explained_variance | 0.856     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 199       |\n",
      "| policy_entropy     | 0.497     |\n",
      "| policy_loss        | 0.019     |\n",
      "| total_timesteps    | 40914918  |\n",
      "| value_loss         | 11.4      |\n",
      "----------------------------------\n",
      "41327208 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 218.59\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 222.91962 |\n",
      "| explained_variance | 0.852     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 200       |\n",
      "| policy_entropy     | 0.495     |\n",
      "| policy_loss        | 0.00189   |\n",
      "| total_timesteps    | 41121559  |\n",
      "| value_loss         | 12.3      |\n",
      "----------------------------------\n",
      "41534640 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.68\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.31105 |\n",
      "| explained_variance | 0.852     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 201       |\n",
      "| policy_entropy     | 0.489     |\n",
      "| policy_loss        | 0.0239    |\n",
      "| total_timesteps    | 41328200  |\n",
      "| value_loss         | 11.8      |\n",
      "----------------------------------\n",
      "41739660 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 221.57\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.75543 |\n",
      "| explained_variance | 0.857     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 202       |\n",
      "| policy_entropy     | 0.474     |\n",
      "| policy_loss        | 0.0773    |\n",
      "| total_timesteps    | 41534841  |\n",
      "| value_loss         | 10.2      |\n",
      "----------------------------------\n",
      "41947092 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.35\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.78732 |\n",
      "| explained_variance | 0.864     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 203       |\n",
      "| policy_entropy     | 0.494     |\n",
      "| policy_loss        | 0.00384   |\n",
      "| total_timesteps    | 41741482  |\n",
      "| value_loss         | 11.1      |\n",
      "----------------------------------\n",
      "42154524 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.25\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 217.87207 |\n",
      "| explained_variance | 0.862     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 204       |\n",
      "| policy_entropy     | 0.495     |\n",
      "| policy_loss        | 0.0058    |\n",
      "| total_timesteps    | 41948123  |\n",
      "| value_loss         | 11.4      |\n",
      "----------------------------------\n",
      "42359544 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.50\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.54256 |\n",
      "| explained_variance | 0.859     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 205       |\n",
      "| policy_entropy     | 0.478     |\n",
      "| policy_loss        | -0.0144   |\n",
      "| total_timesteps    | 42154764  |\n",
      "| value_loss         | 11.2      |\n",
      "----------------------------------\n",
      "42566976 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 221.78\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 223.73976 |\n",
      "| explained_variance | 0.848     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 206       |\n",
      "| policy_entropy     | 0.47      |\n",
      "| policy_loss        | 0.042     |\n",
      "| total_timesteps    | 42361405  |\n",
      "| value_loss         | 11.1      |\n",
      "----------------------------------\n",
      "42774408 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 218.62\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.71233 |\n",
      "| explained_variance | 0.849     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 207       |\n",
      "| policy_entropy     | 0.488     |\n",
      "| policy_loss        | -0.0285   |\n",
      "| total_timesteps    | 42568046  |\n",
      "| value_loss         | 12.4      |\n",
      "----------------------------------\n",
      "42979428 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.43\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 222.63498 |\n",
      "| explained_variance | 0.848     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 208       |\n",
      "| policy_entropy     | 0.473     |\n",
      "| policy_loss        | 0.00105   |\n",
      "| total_timesteps    | 42774687  |\n",
      "| value_loss         | 11.9      |\n",
      "----------------------------------\n",
      "43186860 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.23\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 222.04965 |\n",
      "| explained_variance | 0.862     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 209       |\n",
      "| policy_entropy     | 0.485     |\n",
      "| policy_loss        | -0.00338  |\n",
      "| total_timesteps    | 42981328  |\n",
      "| value_loss         | 10.7      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43394292 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 218.53\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 217.93312 |\n",
      "| explained_variance | 0.854     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 210       |\n",
      "| policy_entropy     | 0.491     |\n",
      "| policy_loss        | -0.0776   |\n",
      "| total_timesteps    | 43187969  |\n",
      "| value_loss         | 12.5      |\n",
      "----------------------------------\n",
      "43599312 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.83\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 222.87772 |\n",
      "| explained_variance | 0.851     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 211       |\n",
      "| policy_entropy     | 0.475     |\n",
      "| policy_loss        | -0.0335   |\n",
      "| total_timesteps    | 43394610  |\n",
      "| value_loss         | 11.9      |\n",
      "----------------------------------\n",
      "43806744 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 218.45\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.71547 |\n",
      "| explained_variance | 0.856     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 212       |\n",
      "| policy_entropy     | 0.491     |\n",
      "| policy_loss        | -0.0678   |\n",
      "| total_timesteps    | 43601251  |\n",
      "| value_loss         | 12.1      |\n",
      "----------------------------------\n",
      "44014176 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.16\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 214.97797 |\n",
      "| explained_variance | 0.854     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 213       |\n",
      "| policy_entropy     | 0.482     |\n",
      "| policy_loss        | -0.00816  |\n",
      "| total_timesteps    | 43807892  |\n",
      "| value_loss         | 11.4      |\n",
      "----------------------------------\n",
      "44219196 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.65\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.72034 |\n",
      "| explained_variance | 0.858     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 214       |\n",
      "| policy_entropy     | 0.478     |\n",
      "| policy_loss        | -0.00598  |\n",
      "| total_timesteps    | 44014533  |\n",
      "| value_loss         | 10.9      |\n",
      "----------------------------------\n",
      "44426628 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 218.85\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 224.32283 |\n",
      "| explained_variance | 0.855     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 215       |\n",
      "| policy_entropy     | 0.485     |\n",
      "| policy_loss        | -0.0747   |\n",
      "| total_timesteps    | 44221174  |\n",
      "| value_loss         | 12.2      |\n",
      "----------------------------------\n",
      "44634060 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.66\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.84789 |\n",
      "| explained_variance | 0.866     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 216       |\n",
      "| policy_entropy     | 0.485     |\n",
      "| policy_loss        | -0.0237   |\n",
      "| total_timesteps    | 44427815  |\n",
      "| value_loss         | 11.1      |\n",
      "----------------------------------\n",
      "44839080 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.70\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.37361 |\n",
      "| explained_variance | 0.848     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 217       |\n",
      "| policy_entropy     | 0.481     |\n",
      "| policy_loss        | -0.0049   |\n",
      "| total_timesteps    | 44634456  |\n",
      "| value_loss         | 12.1      |\n",
      "----------------------------------\n",
      "45046512 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.41\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.05727 |\n",
      "| explained_variance | 0.845     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 218       |\n",
      "| policy_entropy     | 0.48      |\n",
      "| policy_loss        | -0.0378   |\n",
      "| total_timesteps    | 44841097  |\n",
      "| value_loss         | 12.3      |\n",
      "----------------------------------\n",
      "45253944 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.41\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 219.55107 |\n",
      "| explained_variance | 0.851     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 219       |\n",
      "| policy_entropy     | 0.477     |\n",
      "| policy_loss        | -0.00444  |\n",
      "| total_timesteps    | 45047738  |\n",
      "| value_loss         | 11.6      |\n",
      "----------------------------------\n",
      "45458964 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.63\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.19865 |\n",
      "| explained_variance | 0.858     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 220       |\n",
      "| policy_entropy     | 0.486     |\n",
      "| policy_loss        | 0.0225    |\n",
      "| total_timesteps    | 45254379  |\n",
      "| value_loss         | 11.2      |\n",
      "----------------------------------\n",
      "45666396 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 218.55\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 216.88956 |\n",
      "| explained_variance | 0.85      |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 221       |\n",
      "| policy_entropy     | 0.492     |\n",
      "| policy_loss        | -0.0437   |\n",
      "| total_timesteps    | 45461020  |\n",
      "| value_loss         | 12.5      |\n",
      "----------------------------------\n",
      "45873828 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.74\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.61996 |\n",
      "| explained_variance | 0.862     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 222       |\n",
      "| policy_entropy     | 0.493     |\n",
      "| policy_loss        | 0.0122    |\n",
      "| total_timesteps    | 45667661  |\n",
      "| value_loss         | 11.1      |\n",
      "----------------------------------\n",
      "46078848 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.78\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 222.65497 |\n",
      "| explained_variance | 0.85      |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 223       |\n",
      "| policy_entropy     | 0.479     |\n",
      "| policy_loss        | 0.04      |\n",
      "| total_timesteps    | 45874302  |\n",
      "| value_loss         | 11.3      |\n",
      "----------------------------------\n",
      "46286280 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 221.39\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 224.77248 |\n",
      "| explained_variance | 0.862     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 224       |\n",
      "| policy_entropy     | 0.481     |\n",
      "| policy_loss        | 0.0354    |\n",
      "| total_timesteps    | 46080943  |\n",
      "| value_loss         | 10.4      |\n",
      "----------------------------------\n",
      "46493712 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.92\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 216.73491 |\n",
      "| explained_variance | 0.866     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 225       |\n",
      "| policy_entropy     | 0.487     |\n",
      "| policy_loss        | 0.0354    |\n",
      "| total_timesteps    | 46287584  |\n",
      "| value_loss         | 10.8      |\n",
      "----------------------------------\n",
      "46698732 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.57\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.68158 |\n",
      "| explained_variance | 0.856     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 226       |\n",
      "| policy_entropy     | 0.486     |\n",
      "| policy_loss        | -0.0056   |\n",
      "| total_timesteps    | 46494225  |\n",
      "| value_loss         | 11.5      |\n",
      "----------------------------------\n",
      "46906164 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.27\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.10553 |\n",
      "| explained_variance | 0.844     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 227       |\n",
      "| policy_entropy     | 0.489     |\n",
      "| policy_loss        | -0.0217   |\n",
      "| total_timesteps    | 46700866  |\n",
      "| value_loss         | 12.6      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47113596 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.47\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.71758 |\n",
      "| explained_variance | 0.863     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 228       |\n",
      "| policy_entropy     | 0.492     |\n",
      "| policy_loss        | -0.0123   |\n",
      "| total_timesteps    | 46907507  |\n",
      "| value_loss         | 11.3      |\n",
      "----------------------------------\n",
      "47318616 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.20\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 217.08998 |\n",
      "| explained_variance | 0.865     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 229       |\n",
      "| policy_entropy     | 0.492     |\n",
      "| policy_loss        | -0.0323   |\n",
      "| total_timesteps    | 47114148  |\n",
      "| value_loss         | 11.5      |\n",
      "----------------------------------\n",
      "47526048 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.11\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.23924 |\n",
      "| explained_variance | 0.851     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 230       |\n",
      "| policy_entropy     | 0.49      |\n",
      "| policy_loss        | -0.0523   |\n",
      "| total_timesteps    | 47320789  |\n",
      "| value_loss         | 12.2      |\n",
      "----------------------------------\n",
      "47733480 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.58\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 216.61246 |\n",
      "| explained_variance | 0.846     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 231       |\n",
      "| policy_entropy     | 0.478     |\n",
      "| policy_loss        | 0.00707   |\n",
      "| total_timesteps    | 47527430  |\n",
      "| value_loss         | 12.1      |\n",
      "----------------------------------\n",
      "47938500 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.21\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 217.74757 |\n",
      "| explained_variance | 0.845     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 232       |\n",
      "| policy_entropy     | 0.486     |\n",
      "| policy_loss        | -0.0359   |\n",
      "| total_timesteps    | 47734071  |\n",
      "| value_loss         | 12.6      |\n",
      "----------------------------------\n",
      "48145932 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 221.06\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.42377 |\n",
      "| explained_variance | 0.851     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 233       |\n",
      "| policy_entropy     | 0.477     |\n",
      "| policy_loss        | 0.0114    |\n",
      "| total_timesteps    | 47940712  |\n",
      "| value_loss         | 11.4      |\n",
      "----------------------------------\n",
      "48353364 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.99\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 217.25449 |\n",
      "| explained_variance | 0.855     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 234       |\n",
      "| policy_entropy     | 0.477     |\n",
      "| policy_loss        | 0.00228   |\n",
      "| total_timesteps    | 48147353  |\n",
      "| value_loss         | 11.3      |\n",
      "----------------------------------\n",
      "48558384 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.35\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 220.35242 |\n",
      "| explained_variance | 0.851     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 235       |\n",
      "| policy_entropy     | 0.48      |\n",
      "| policy_loss        | -0.0108   |\n",
      "| total_timesteps    | 48353994  |\n",
      "| value_loss         | 11.6      |\n",
      "----------------------------------\n",
      "48765816 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 218.44\n",
      "---------------------------------\n",
      "| ep_len_mean        | 201      |\n",
      "| ep_reward_mean     | 213.831  |\n",
      "| explained_variance | 0.858    |\n",
      "| fps                | 3197     |\n",
      "| nupdates           | 236      |\n",
      "| policy_entropy     | 0.498    |\n",
      "| policy_loss        | -0.0529  |\n",
      "| total_timesteps    | 48560635 |\n",
      "| value_loss         | 12.1     |\n",
      "---------------------------------\n",
      "48973248 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 219.92\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 215.85022 |\n",
      "| explained_variance | 0.852     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 237       |\n",
      "| policy_entropy     | 0.481     |\n",
      "| policy_loss        | 0.011     |\n",
      "| total_timesteps    | 48767276  |\n",
      "| value_loss         | 11.8      |\n",
      "----------------------------------\n",
      "49178268 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 221.29\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 222.95343 |\n",
      "| explained_variance | 0.877     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 238       |\n",
      "| policy_entropy     | 0.483     |\n",
      "| policy_loss        | 0.0339    |\n",
      "| total_timesteps    | 48973917  |\n",
      "| value_loss         | 9.54      |\n",
      "----------------------------------\n",
      "49385700 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.80\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 217.37048 |\n",
      "| explained_variance | 0.857     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 239       |\n",
      "| policy_entropy     | 0.48      |\n",
      "| policy_loss        | 0.0261    |\n",
      "| total_timesteps    | 49180558  |\n",
      "| value_loss         | 11        |\n",
      "----------------------------------\n",
      "49593132 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.27\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 218.04555 |\n",
      "| explained_variance | 0.858     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 240       |\n",
      "| policy_entropy     | 0.484     |\n",
      "| policy_loss        | 0.024     |\n",
      "| total_timesteps    | 49387199  |\n",
      "| value_loss         | 10.8      |\n",
      "----------------------------------\n",
      "49798152 timesteps\n",
      "Best mean reward: 222.05 - Last mean reward per episode: 220.94\n",
      "----------------------------------\n",
      "| ep_len_mean        | 201       |\n",
      "| ep_reward_mean     | 221.54437 |\n",
      "| explained_variance | 0.848     |\n",
      "| fps                | 3197      |\n",
      "| nupdates           | 241       |\n",
      "| policy_entropy     | 0.475     |\n",
      "| policy_loss        | 0.0341    |\n",
      "| total_timesteps    | 49593840  |\n",
      "| value_loss         | 11.5      |\n",
      "----------------------------------\n",
      "done\n",
      "predict...\n",
      "simulating  saved/best_perus\n",
      "predicting...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4272503a74c642c891090c2482fb4498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Population', max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6535142809866791\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_emp 0\n",
      "Vaikutus työllisyyteen keskiarvo 2178314.0 htv, mediaani 2178314.0 htv\n",
      "                        keskiarvo 2298628.0 työllistä, mediaani 2298628.0 työllistä\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cc1=Lifecycle(env='unemployment-v1',minimal=False,mortality=mortality,perustulo=False,\n",
    "              deterministic=deterministic,randomness=randomness)\n",
    "\n",
    "cc1.explain()\n",
    "cc1.run_results(debug=False,steps1=size1,steps2=size2,pop=pop_size,deterministic=deterministic,\n",
    "                train=True,predict=True,batch1=batch1,batch2=batch2,bestname='saved/best_perus',\n",
    "                plot=True,save='saved/perusmalli',cont=False,start_from='saved/best_perus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EK:n malli\n",
    "\n",
    "Lasketaan vertailukelpoiset työllisyysasteet EK:n ehdottamalla mallilla. Mallissa on toteuttu muutoksia ansiosidonnaiseen työttömyysturvaan, asumistukeen, toimeentulotukeen ja verotukseen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No mortality included\n",
      "train...\n",
      "phase 1\n",
      "training...\n",
      "---------------------------------\n",
      "| explained_variance | 0.000587 |\n",
      "| fps                | 1807     |\n",
      "| nupdates           | 1        |\n",
      "| policy_entropy     | 1.39     |\n",
      "| policy_loss        | 53.1     |\n",
      "| total_timesteps    | 0        |\n",
      "| value_loss         | 1.58e+03 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.959    |\n",
      "| fps                | 2299     |\n",
      "| nupdates           | 1000     |\n",
      "| policy_entropy     | 0.95     |\n",
      "| policy_loss        | 0.313    |\n",
      "| total_timesteps    | 2458539  |\n",
      "| value_loss         | 3.92     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.954    |\n",
      "| fps                | 2311     |\n",
      "| nupdates           | 2000     |\n",
      "| policy_entropy     | 0.688    |\n",
      "| policy_loss        | 0.0515   |\n",
      "| total_timesteps    | 4919539  |\n",
      "| value_loss         | 1.67     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.972    |\n",
      "| fps                | 2316     |\n",
      "| nupdates           | 3000     |\n",
      "| policy_entropy     | 0.493    |\n",
      "| policy_loss        | -0.0988  |\n",
      "| total_timesteps    | 7380539  |\n",
      "| value_loss         | 2.39     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.942    |\n",
      "| fps                | 2302     |\n",
      "| nupdates           | 4000     |\n",
      "| policy_entropy     | 0.392    |\n",
      "| policy_loss        | -0.00995 |\n",
      "| total_timesteps    | 9841539  |\n",
      "| value_loss         | 5.41     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.808    |\n",
      "| fps                | 2232     |\n",
      "| nupdates           | 5000     |\n",
      "| policy_entropy     | 0.418    |\n",
      "| policy_loss        | -0.16    |\n",
      "| total_timesteps    | 12302539 |\n",
      "| value_loss         | 21.7     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "cc2=Lifecycle(env='unemploymentEK-v1',minimal=False,mortality=mortality,perustulo=False,\n",
    "              randomness=randomness)\n",
    "cc2.run_results(debug=False,steps1=size1,steps2=size2,pop=pop_size,deterministic=deterministic,\n",
    "                train=True,predict=True,batch1=batch1,batch2=batch2,bestname='saved/best_ek',plot=True,\n",
    "                save='saved/best_ek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#cc1.simulate(pop=10_000,deterministic=True,load='best/best_perus',rlmodel='acktr',save='perusmalli',modify_load=False)\n",
    "#cc2.simulate(pop=10_000,deterministic=True,load='best/best_ek',rlmodel='acktr',save='eknmalli',modify_load=False)\n",
    "cc2.compare_with(cc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Verifiointi\n",
    "\n",
    "Ajetaan sama simulaatio kaksi kertaa ja tarkastetaan että tulokset ovat sama. Tässä ideana on varmistaa, että satunnaisuus ei sotke tulosten arviointia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Työssäoloehdon pituus 12 kk\n",
    "\n",
    "Entä jos työssäoloehto olisikin 12 kuukautta pitkä nykyisen 6 kuukauden sijaan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cc1_toe=Lifecycle(env='unemployment-v1',minimal=False,ansiopvraha_toe=1.0,mortality=mortality,\n",
    "                  perustulo=False,randomness=randomness)\n",
    "cc1_toe.run_results(debug=False,steps1=size1,steps2=size2,pop=pop_size,deterministic=deterministic,\n",
    "                    train=True,predict=True,batch1=batch1,batch2=batch2,bestname='best_12kk',plot=True,\n",
    "                    save='saved/malli_12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc1_toe.compare_with(cc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ansiosidonnaisen päivärahan lyhennys 50 pv\n",
    "\n",
    "Tarkastellaan, miten työllisyyteen vaikuttaisi ansiosidonnaisen päivärahan lyhentäminen 50 päivällä. Tällöin alle kolmen vuoden työhistorialla ansiosidonnaisen päivärahan enimmäiskesto olisi 250 pv ja pidemmällä työhistorialla enimmäiskesto olisi 350 pv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cc1_350=Lifecycle(env='unemployment-v1',minimal=False,ansiopvraha_kesto300=250,ansiopvraha_kesto400=350,\n",
    "                  mortality=mortality,perustulo=False,randomness=randomness)\n",
    "cc1_350.run_results(debug=False,steps1=size1,steps2=size2,pop=pop_size,deterministic=deterministic,\n",
    "                    train=True,predict=True,batch1=batch1,batch2=batch2,bestname='best_50pv',plot=True,\n",
    "                    save='saved/malli_ek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc1_350.compare_with(cc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Työttömyysputken poisto\n",
    "\n",
    "Työttömyysputki on suosittu elinkaarimalleissa. Tarkastellaan millainen työllisyysvaikutus on putken poistamisella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc1_putki=Lifecycle(env='unemployment-v1',minimal=False,include_putki=False,mortality=mortality,\n",
    "                    perustulo=False,randomness=randomness)\n",
    "cc1_putki.run_results(debug=False,steps1=size1,steps2=size2,pop=pop_size,deterministic=deterministic,\n",
    "                      train=True,predict=True,batch1=batch1,batch2=batch2,bestname='best_putki',\n",
    "                      plot=True,save='saved/malli_putki')\n",
    "cc1_putki.compare_with(cc1)                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perustulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cc1_pt=Lifecycle(env='unemployment-v1',minimal=False,perustulo=True,mortality=mortality,\n",
    "                 randomness=randomness)\n",
    "cc1_pt.run_results(debug=False,steps1=size1,steps2=size2,pop=pop_size,deterministic=deterministic,\n",
    "                   train=True,predict=True,batch1=batch1,batch2=batch2,bestname='best_pt',plot=True,\n",
    "                   save='saved/malli_pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc1_pt.compare_with(cc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 300 vs 400 päivän kesto ansiosidonnaisessa\n",
    "\n",
    "Mikä on alle kolmen vuoden työhistorian lyhyemmän (300 pv) ansiosidonnaisen päivärahan enimmäiskeston vaikutus työllisyyteen? Kokeillaan miten työllisyyteen vaikuttaisi sen pidentäminen 400 päivään."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cc1_400=Lifecycle(env='unemployment-v1',minimal=False,ansiopvraha_kesto300=400,mortality=mortality,\n",
    "                  perustulo=False,randomness=randomness)\n",
    "cc1_400.run_results(debug=False,steps1=size1,steps2=size2,pop=pop_size,deterministic=deterministic,\n",
    "                    train=True,predict=True,batch1=batch1,batch2=batch2,bestname='best_300pv',plot=True,\n",
    "                    save='saved/malli_300')\n",
    "cc1_400.compare_with(cc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
